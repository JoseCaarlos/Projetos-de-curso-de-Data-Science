{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V_aQUXqWdhaT"
   },
   "source": [
    "# Projeto PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EiFVRJb9eH93"
   },
   "source": [
    "## Etapa 1: Importação das bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5414,
     "status": "ok",
     "timestamp": 1595022453897,
     "user": {
      "displayName": "Matheus Pereira de Novaes",
      "photoUrl": "",
      "userId": "01574286574804862444"
     },
     "user_tz": 180
    },
    "id": "Q13dYYT7vDD3",
    "outputId": "818a9129-23fb-4210-e617-0b4c44239754"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import torch \n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SaiWxos9htu9"
   },
   "source": [
    "## Etapa 2: Importação da base dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 875,
     "status": "ok",
     "timestamp": 1595022597288,
     "user": {
      "displayName": "Matheus Pereira de Novaes",
      "photoUrl": "",
      "userId": "01574286574804862444"
     },
     "user_tz": 180
    },
    "id": "1QSXhsnjwAwv",
    "outputId": "781009c2-681e-4ef3-e69b-b6698c9d5ce1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x234cdf97d10>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 798,
     "status": "ok",
     "timestamp": 1595022678112,
     "user": {
      "displayName": "Matheus Pereira de Novaes",
      "photoUrl": "",
      "userId": "01574286574804862444"
     },
     "user_tz": 180
    },
    "id": "RYcwea5uwYPr"
   },
   "outputs": [],
   "source": [
    "datasetInput = pd.read_csv('entradas_breast.csv')\n",
    "datasetOuput = pd.read_csv('saidas_breast.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 218
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2136,
     "status": "ok",
     "timestamp": 1595022709385,
     "user": {
      "displayName": "Matheus Pereira de Novaes",
      "photoUrl": "",
      "userId": "01574286574804862444"
     },
     "user_tz": 180
    },
    "id": "8KUkSL98wq4G",
    "outputId": "8190c667-6a06-4eb8-b5ad-1f90c3f7a65f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave_points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>fractal_dimension_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave_points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>186.0000</td>\n",
       "      <td>275.0000</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>243.0000</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>173.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>198.0000</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>205.0000</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    radius_mean   texture_mean   perimeter_mean   area_mean   smoothness_mean  \\\n",
       "0         17.99          10.38           122.80      1001.0           0.11840   \n",
       "1         20.57          17.77           132.90      1326.0           0.08474   \n",
       "2         19.69          21.25           130.00      1203.0           0.10960   \n",
       "3         11.42          20.38            77.58       386.1           0.14250   \n",
       "4         20.29          14.34           135.10      1297.0           0.10030   \n",
       "\n",
       "    compactness_mean   concavity_mean  concave_points_mean   symmetry_mean  \\\n",
       "0            0.27760           0.3001              0.14710          0.2419   \n",
       "1            0.07864           0.0869              0.07017          0.1812   \n",
       "2            0.15990           0.1974              0.12790          0.2069   \n",
       "3            0.28390           0.2414              0.10520          0.2597   \n",
       "4            0.13280         198.0000              0.10430          0.1809   \n",
       "\n",
       "    fractal_dimension_mean  ...   radius_worst   texture_worst  \\\n",
       "0                  0.07871  ...          25.38           17.33   \n",
       "1                  0.05667  ...          24.99           23.41   \n",
       "2                  0.05999  ...          23.57           25.53   \n",
       "3                  0.09744  ...          14.91           26.50   \n",
       "4                  0.05883  ...          22.54           16.67   \n",
       "\n",
       "    perimeter_worst   area_worst   smoothness_worst   compactness_worst  \\\n",
       "0            184.60       2019.0             0.1622              0.6656   \n",
       "1            158.80       1956.0             0.1238              0.1866   \n",
       "2            152.50       1709.0             0.1444              0.4245   \n",
       "3             98.87        567.7             0.2098              0.8663   \n",
       "4            152.20       1575.0             0.1374            205.0000   \n",
       "\n",
       "    concavity_worst   concave_points_worst   symmetry_worst  \\\n",
       "0            0.7119                 0.2654           0.4601   \n",
       "1            0.2416               186.0000         275.0000   \n",
       "2            0.4504               243.0000           0.3613   \n",
       "3            0.6869                 0.2575           0.6638   \n",
       "4            0.4000                 0.1625           0.2364   \n",
       "\n",
       "    fractal_dimension_worst  \n",
       "0                   0.11890  \n",
       "1                   0.08902  \n",
       "2                   0.08758  \n",
       "3                 173.00000  \n",
       "4                   0.07678  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasetInput.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 198
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1185,
     "status": "ok",
     "timestamp": 1595022788017,
     "user": {
      "displayName": "Matheus Pereira de Novaes",
      "photoUrl": "",
      "userId": "01574286574804862444"
     },
     "user_tz": 180
    },
    "id": "W3J4TwDexBgA",
    "outputId": "c8056c48-4b75-4aaf-9e61-8f8791e4d886"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0\n",
       "0  0\n",
       "1  0\n",
       "2  0\n",
       "3  0\n",
       "4  0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasetOuput.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 774,
     "status": "ok",
     "timestamp": 1595022857185,
     "user": {
      "displayName": "Matheus Pereira de Novaes",
      "photoUrl": "",
      "userId": "01574286574804862444"
     },
     "user_tz": 180
    },
    "id": "ZEBhpoTCxRJR",
    "outputId": "060fb677-ca0d-4751-b18e-ac444c6e706e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1], dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(datasetOuput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 802,
     "status": "ok",
     "timestamp": 1595022912308,
     "user": {
      "displayName": "Matheus Pereira de Novaes",
      "photoUrl": "",
      "userId": "01574286574804862444"
     },
     "user_tz": 180
    },
    "id": "nDi3YHr9xYzo",
    "outputId": "e0b44110-bb31-4058-f78a-2873c186c73b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x234d0292d48>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizando as duas classes\n",
    "sns.countplot(datasetOuput['0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1195,
     "status": "ok",
     "timestamp": 1595023643730,
     "user": {
      "displayName": "Matheus Pereira de Novaes",
      "photoUrl": "",
      "userId": "01574286574804862444"
     },
     "user_tz": 180
    },
    "id": "GdunHu_BxylZ"
   },
   "outputs": [],
   "source": [
    "# Separando nossos dados de Treino e Teste\n",
    "X_treinamento,X_teste,y_treinamento,y_teste = train_test_split(datasetInput,datasetOuput,test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 776,
     "status": "ok",
     "timestamp": 1595023147923,
     "user": {
      "displayName": "Matheus Pereira de Novaes",
      "photoUrl": "",
      "userId": "01574286574804862444"
     },
     "user_tz": 180
    },
    "id": "nb66foB7yWVB",
    "outputId": "ef293139-ce6d-43bd-ccfb-a254d509b223"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(426, 30)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_treinamento.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 997,
     "status": "ok",
     "timestamp": 1595023181651,
     "user": {
      "displayName": "Matheus Pereira de Novaes",
      "photoUrl": "",
      "userId": "01574286574804862444"
     },
     "user_tz": 180
    },
    "id": "A8oXwD-6ygQZ",
    "outputId": "bb1a54d9-3b60-4971-8ce7-e4bee0d5c51a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(143, 30)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_teste.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RxVXWL_Cdfhf"
   },
   "source": [
    "## Etapa 3: Transformação dos dados para tensores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 949,
     "status": "ok",
     "timestamp": 1595023287672,
     "user": {
      "displayName": "Matheus Pereira de Novaes",
      "photoUrl": "",
      "userId": "01574286574804862444"
     },
     "user_tz": 180
    },
    "id": "CSp6bN8ly6Yi",
    "outputId": "d93d761f-793e-443f-d6aa-a25df38f9fc1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_treinamento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 738,
     "status": "ok",
     "timestamp": 1595023377754,
     "user": {
      "displayName": "Matheus Pereira de Novaes",
      "photoUrl": "",
      "userId": "01574286574804862444"
     },
     "user_tz": 180
    },
    "id": "G4MB-virzIz8",
    "outputId": "44fa3f5a-6479-4bba-cb6e-a395542bf892"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(np.array(X_treinamento))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 963,
     "status": "ok",
     "timestamp": 1595023650958,
     "user": {
      "displayName": "Matheus Pereira de Novaes",
      "photoUrl": "",
      "userId": "01574286574804862444"
     },
     "user_tz": 180
    },
    "id": "GOcpqGQkzYUv"
   },
   "outputs": [],
   "source": [
    "X_treinamento = torch.tensor(np.array(X_treinamento),dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1119,
     "status": "ok",
     "timestamp": 1595023654749,
     "user": {
      "displayName": "Matheus Pereira de Novaes",
      "photoUrl": "",
      "userId": "01574286574804862444"
     },
     "user_tz": 180
    },
    "id": "p2Db8x4Fz44j",
    "outputId": "b8e8de40-5079-40b6-d4d4-95f234c9d648"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([426, 30])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_treinamento.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 254
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1280,
     "status": "ok",
     "timestamp": 1595023662506,
     "user": {
      "displayName": "Matheus Pereira de Novaes",
      "photoUrl": "",
      "userId": "01574286574804862444"
     },
     "user_tz": 180
    },
    "id": "1wkvRu6G0Nwp",
    "outputId": "02c35a36-e91d-4089-8186-af1a3bed2faf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.1540e+01, 1.4440e+01, 7.4650e+01,  ..., 6.9180e-02, 2.3290e-01,\n",
       "         8.1340e-02],\n",
       "        [2.0310e+01, 2.7060e+01, 1.3290e+02,  ..., 1.6970e-01, 3.1510e-01,\n",
       "         7.9990e-02],\n",
       "        [1.1360e+01, 1.7570e+01, 7.2490e+01,  ..., 8.6980e-02, 2.9730e-01,\n",
       "         7.7450e-02],\n",
       "        ...,\n",
       "        [1.2050e+01, 2.2720e+01, 7.8750e+01,  ..., 1.0920e-01, 2.1910e-01,\n",
       "         9.3490e-02],\n",
       "        [2.0440e+01, 2.1780e+01, 1.3380e+02,  ..., 1.7650e-01, 2.6090e-01,\n",
       "         6.7350e-02],\n",
       "        [1.1740e+01, 1.4690e+01, 7.6310e+01,  ..., 1.0560e-01, 2.6040e-01,\n",
       "         9.8790e-02]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 950,
     "status": "ok",
     "timestamp": 1595023710163,
     "user": {
      "displayName": "Matheus Pereira de Novaes",
      "photoUrl": "",
      "userId": "01574286574804862444"
     },
     "user_tz": 180
    },
    "id": "EBHyECEt0gKo"
   },
   "outputs": [],
   "source": [
    "y_treinamento = torch.tensor(np.array(y_treinamento),dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1600,
     "status": "ok",
     "timestamp": 1595023831186,
     "user": {
      "displayName": "Matheus Pereira de Novaes",
      "photoUrl": "",
      "userId": "01574286574804862444"
     },
     "user_tz": 180
    },
    "id": "HbyiJXUT0pfz"
   },
   "outputs": [],
   "source": [
    "dataset = torch.utils.data.TensorDataset(X_treinamento,y_treinamento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 753,
     "status": "ok",
     "timestamp": 1595023848359,
     "user": {
      "displayName": "Matheus Pereira de Novaes",
      "photoUrl": "",
      "userId": "01574286574804862444"
     },
     "user_tz": 180
    },
    "id": "zLCag_xv1Evp",
    "outputId": "9962ef9d-328e-4a18-e847-6d8cfaf8b6bf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.utils.data.dataset.TensorDataset"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 813,
     "status": "ok",
     "timestamp": 1595024107955,
     "user": {
      "displayName": "Matheus Pereira de Novaes",
      "photoUrl": "",
      "userId": "01574286574804862444"
     },
     "user_tz": 180
    },
    "id": "oedaEPZx1eeX"
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NEN6Y5KZiKgr"
   },
   "source": [
    "## Etapa 4: Construção da rede neural\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 967,
     "status": "ok",
     "timestamp": 1595024499509,
     "user": {
      "displayName": "Matheus Pereira de Novaes",
      "photoUrl": "",
      "userId": "01574286574804862444"
     },
     "user_tz": 180
    },
    "id": "gW_cs6UR2XyA"
   },
   "outputs": [],
   "source": [
    "#30 -> 16 -> 16 -> 1\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(in_features = 30,out_features=16),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(16,16),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(16,1),\n",
    "    nn.Sigmoid() # Retorna valor entre 0 e 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 163
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1015,
     "status": "ok",
     "timestamp": 1595024517611,
     "user": {
      "displayName": "Matheus Pereira de Novaes",
      "photoUrl": "",
      "userId": "01574286574804862444"
     },
     "user_tz": 180
    },
    "id": "01gDPT5k3nZN",
    "outputId": "e34eca1d-9d9d-4517-adf3-2a786fe03798"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of Sequential(\n",
       "  (0): Linear(in_features=30, out_features=16, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=16, out_features=16, bias=True)\n",
       "  (3): ReLU()\n",
       "  (4): Linear(in_features=16, out_features=1, bias=True)\n",
       "  (5): Sigmoid()\n",
       ")>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 739,
     "status": "ok",
     "timestamp": 1595024778632,
     "user": {
      "displayName": "Matheus Pereira de Novaes",
      "photoUrl": "",
      "userId": "01574286574804862444"
     },
     "user_tz": 180
    },
    "id": "XkeVCiRP34zU"
   },
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1204,
     "status": "ok",
     "timestamp": 1595024776720,
     "user": {
      "displayName": "Matheus Pereira de Novaes",
      "photoUrl": "",
      "userId": "01574286574804862444"
     },
     "user_tz": 180
    },
    "id": "2BuK28wA4Hhs"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IWkLhsOAiPBk"
   },
   "source": [
    "## Etapa 5: Treinamento da rede"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 42382,
     "status": "ok",
     "timestamp": 1595025492020,
     "user": {
      "displayName": "Matheus Pereira de Novaes",
      "photoUrl": "",
      "userId": "01574286574804862444"
     },
     "user_tz": 180
    },
    "id": "MsxRPEZJ44oz",
    "outputId": "9b99691e-8d93-469f-b022-791c02ac26ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Época 1 : perda 10.99630\n",
      "Época 2 : perda 3.80302\n",
      "Época 3 : perda 3.57072\n",
      "Época 4 : perda 3.41478\n",
      "Época 5 : perda 3.38310\n",
      "Época 6 : perda 3.35264\n",
      "Época 7 : perda 3.35849\n",
      "Época 8 : perda 3.34306\n",
      "Época 9 : perda 3.31429\n",
      "Época 10 : perda 3.31778\n",
      "Época 11 : perda 3.27551\n",
      "Época 12 : perda 3.28760\n",
      "Época 13 : perda 3.25985\n",
      "Época 14 : perda 3.27704\n",
      "Época 15 : perda 3.44482\n",
      "Época 16 : perda 3.27046\n",
      "Época 17 : perda 3.38948\n",
      "Época 18 : perda 3.23442\n",
      "Época 19 : perda 3.38343\n",
      "Época 20 : perda 3.22839\n",
      "Época 21 : perda 3.25533\n",
      "Época 22 : perda 3.32165\n",
      "Época 23 : perda 3.24175\n",
      "Época 24 : perda 3.19307\n",
      "Época 25 : perda 3.24402\n",
      "Época 26 : perda 3.18710\n",
      "Época 27 : perda 3.19956\n",
      "Época 28 : perda 3.19162\n",
      "Época 29 : perda 3.19148\n",
      "Época 30 : perda 3.17770\n",
      "Época 31 : perda 3.21046\n",
      "Época 32 : perda 3.18379\n",
      "Época 33 : perda 3.18577\n",
      "Época 34 : perda 3.19695\n",
      "Época 35 : perda 3.24441\n",
      "Época 36 : perda 3.24656\n",
      "Época 37 : perda 3.26068\n",
      "Época 38 : perda 3.17016\n",
      "Época 39 : perda 3.20712\n",
      "Época 40 : perda 3.22240\n",
      "Época 41 : perda 3.17674\n",
      "Época 42 : perda 3.33192\n",
      "Época 43 : perda 3.17190\n",
      "Época 44 : perda 3.21136\n",
      "Época 45 : perda 3.17986\n",
      "Época 46 : perda 3.31918\n",
      "Época 47 : perda 3.16214\n",
      "Época 48 : perda 3.17650\n",
      "Época 49 : perda 3.20729\n",
      "Época 50 : perda 3.32627\n",
      "Época 51 : perda 3.16569\n",
      "Época 52 : perda 3.17948\n",
      "Época 53 : perda 3.15871\n",
      "Época 54 : perda 3.14040\n",
      "Época 55 : perda 3.30570\n",
      "Época 56 : perda 3.14019\n",
      "Época 57 : perda 3.30975\n",
      "Época 58 : perda 3.16570\n",
      "Época 59 : perda 3.15670\n",
      "Época 60 : perda 3.18962\n",
      "Época 61 : perda 3.14996\n",
      "Época 62 : perda 3.15769\n",
      "Época 63 : perda 3.17877\n",
      "Época 64 : perda 3.15437\n",
      "Época 65 : perda 3.14774\n",
      "Época 66 : perda 3.16432\n",
      "Época 67 : perda 3.15533\n",
      "Época 68 : perda 3.15445\n",
      "Época 69 : perda 3.13790\n",
      "Época 70 : perda 3.13976\n",
      "Época 71 : perda 3.13541\n",
      "Época 72 : perda 3.12742\n",
      "Época 73 : perda 3.46428\n",
      "Época 74 : perda 3.33052\n",
      "Época 75 : perda 3.16364\n",
      "Época 76 : perda 3.12404\n",
      "Época 77 : perda 3.12644\n",
      "Época 78 : perda 3.13083\n",
      "Época 79 : perda 3.17380\n",
      "Época 80 : perda 3.15136\n",
      "Época 81 : perda 3.12183\n",
      "Época 82 : perda 3.13616\n",
      "Época 83 : perda 3.14465\n",
      "Época 84 : perda 3.18215\n",
      "Época 85 : perda 3.15986\n",
      "Época 86 : perda 3.12751\n",
      "Época 87 : perda 3.12343\n",
      "Época 88 : perda 3.30372\n",
      "Época 89 : perda 3.14036\n",
      "Época 90 : perda 3.20265\n",
      "Época 91 : perda 3.15987\n",
      "Época 92 : perda 3.16302\n",
      "Época 93 : perda 3.14674\n",
      "Época 94 : perda 3.10947\n",
      "Época 95 : perda 3.10774\n",
      "Época 96 : perda 3.11521\n",
      "Época 97 : perda 3.12474\n",
      "Época 98 : perda 3.18625\n",
      "Época 99 : perda 3.13037\n",
      "Época 100 : perda 3.12401\n",
      "Época 101 : perda 3.12797\n",
      "Época 102 : perda 3.13283\n",
      "Época 103 : perda 3.28382\n",
      "Época 104 : perda 3.14212\n",
      "Época 105 : perda 3.11947\n",
      "Época 106 : perda 3.12995\n",
      "Época 107 : perda 3.13590\n",
      "Época 108 : perda 3.12191\n",
      "Época 109 : perda 3.10905\n",
      "Época 110 : perda 3.41983\n",
      "Época 111 : perda 3.12582\n",
      "Época 112 : perda 3.12114\n",
      "Época 113 : perda 3.27778\n",
      "Época 114 : perda 3.09510\n",
      "Época 115 : perda 3.11923\n",
      "Época 116 : perda 3.12446\n",
      "Época 117 : perda 3.12253\n",
      "Época 118 : perda 3.26681\n",
      "Época 119 : perda 3.26685\n",
      "Época 120 : perda 3.12843\n",
      "Época 121 : perda 3.10736\n",
      "Época 122 : perda 3.13726\n",
      "Época 123 : perda 3.10393\n",
      "Época 124 : perda 3.09865\n",
      "Época 125 : perda 3.25396\n",
      "Época 126 : perda 3.11850\n",
      "Época 127 : perda 3.25983\n",
      "Época 128 : perda 3.28202\n",
      "Época 129 : perda 3.10813\n",
      "Época 130 : perda 3.30827\n",
      "Época 131 : perda 3.20404\n",
      "Época 132 : perda 3.11445\n",
      "Época 133 : perda 3.10365\n",
      "Época 134 : perda 3.12834\n",
      "Época 135 : perda 3.12847\n",
      "Época 136 : perda 3.14414\n",
      "Época 137 : perda 3.11595\n",
      "Época 138 : perda 3.11755\n",
      "Época 139 : perda 3.09423\n",
      "Época 140 : perda 3.26341\n",
      "Época 141 : perda 3.15084\n",
      "Época 142 : perda 3.25785\n",
      "Época 143 : perda 3.11402\n",
      "Época 144 : perda 3.16356\n",
      "Época 145 : perda 3.13603\n",
      "Época 146 : perda 3.27154\n",
      "Época 147 : perda 3.11307\n",
      "Época 148 : perda 3.14356\n",
      "Época 149 : perda 3.10650\n",
      "Época 150 : perda 3.11599\n",
      "Época 151 : perda 3.13731\n",
      "Época 152 : perda 3.19371\n",
      "Época 153 : perda 1.65824\n",
      "Época 154 : perda 1.33843\n",
      "Época 155 : perda 1.47738\n",
      "Época 156 : perda 1.31176\n",
      "Época 157 : perda 1.28395\n",
      "Época 158 : perda 1.25574\n",
      "Época 159 : perda 1.25948\n",
      "Época 160 : perda 1.06044\n",
      "Época 161 : perda 0.42572\n",
      "Época 162 : perda 0.59866\n",
      "Época 163 : perda 0.40595\n",
      "Época 164 : perda 0.58751\n",
      "Época 165 : perda 0.56781\n",
      "Época 166 : perda 0.55467\n",
      "Época 167 : perda 0.54650\n",
      "Época 168 : perda 0.55346\n",
      "Época 169 : perda 0.56502\n",
      "Época 170 : perda 0.43487\n",
      "Época 171 : perda 0.42630\n",
      "Época 172 : perda 0.56809\n",
      "Época 173 : perda 0.55239\n",
      "Época 174 : perda 0.57087\n",
      "Época 175 : perda 0.58375\n",
      "Época 176 : perda 0.58845\n",
      "Época 177 : perda 0.76351\n",
      "Época 178 : perda 0.54185\n",
      "Época 179 : perda 0.55062\n",
      "Época 180 : perda 0.54634\n",
      "Época 181 : perda 0.54814\n",
      "Época 182 : perda 0.55154\n",
      "Época 183 : perda 0.56942\n",
      "Época 184 : perda 0.54268\n",
      "Época 185 : perda 0.57084\n",
      "Época 186 : perda 0.59977\n",
      "Época 187 : perda 0.57611\n",
      "Época 188 : perda 0.54442\n",
      "Época 189 : perda 0.57182\n",
      "Época 190 : perda 0.55211\n",
      "Época 191 : perda 0.52958\n",
      "Época 192 : perda 0.54038\n",
      "Época 193 : perda 0.53093\n",
      "Época 194 : perda 0.53964\n",
      "Época 195 : perda 0.54752\n",
      "Época 196 : perda 0.53936\n",
      "Época 197 : perda 0.54720\n",
      "Época 198 : perda 0.58501\n",
      "Época 199 : perda 0.55931\n",
      "Época 200 : perda 0.56232\n",
      "Época 201 : perda 0.53830\n",
      "Época 202 : perda 0.54996\n",
      "Época 203 : perda 0.54195\n",
      "Época 204 : perda 0.55662\n",
      "Época 205 : perda 0.53576\n",
      "Época 206 : perda 0.52840\n",
      "Época 207 : perda 0.56289\n",
      "Época 208 : perda 0.54130\n",
      "Época 209 : perda 0.56348\n",
      "Época 210 : perda 0.56977\n",
      "Época 211 : perda 0.56944\n",
      "Época 212 : perda 0.54712\n",
      "Época 213 : perda 0.54012\n",
      "Época 214 : perda 0.55428\n",
      "Época 215 : perda 0.59106\n",
      "Época 216 : perda 0.59198\n",
      "Época 217 : perda 0.56260\n",
      "Época 218 : perda 0.57061\n",
      "Época 219 : perda 0.55489\n",
      "Época 220 : perda 0.53716\n",
      "Época 221 : perda 0.52458\n",
      "Época 222 : perda 0.52283\n",
      "Época 223 : perda 0.53537\n",
      "Época 224 : perda 0.54390\n",
      "Época 225 : perda 0.59048\n",
      "Época 226 : perda 0.55116\n",
      "Época 227 : perda 0.54238\n",
      "Época 228 : perda 0.38247\n",
      "Época 229 : perda 0.46538\n",
      "Época 230 : perda 0.53651\n",
      "Época 231 : perda 0.18737\n",
      "Época 232 : perda 0.10575\n",
      "Época 233 : perda 0.08961\n",
      "Época 234 : perda 0.10243\n",
      "Época 235 : perda 0.10445\n",
      "Época 236 : perda 0.07119\n",
      "Época 237 : perda 0.07357\n",
      "Época 238 : perda 0.07211\n",
      "Época 239 : perda 0.10548\n",
      "Época 240 : perda 0.06350\n",
      "Época 241 : perda 0.08949\n",
      "Época 242 : perda 0.07303\n",
      "Época 243 : perda 0.08441\n",
      "Época 244 : perda 0.06099\n",
      "Época 245 : perda 0.06769\n",
      "Época 246 : perda 0.07444\n",
      "Época 247 : perda 0.10740\n",
      "Época 248 : perda 0.06696\n",
      "Época 249 : perda 0.06893\n",
      "Época 250 : perda 0.10838\n",
      "Época 251 : perda 0.08602\n",
      "Época 252 : perda 0.09399\n",
      "Época 253 : perda 0.07865\n",
      "Época 254 : perda 0.08049\n",
      "Época 255 : perda 0.07551\n",
      "Época 256 : perda 0.07681\n",
      "Época 257 : perda 0.07963\n",
      "Época 258 : perda 0.08396\n",
      "Época 259 : perda 0.06868\n",
      "Época 260 : perda 0.10190\n",
      "Época 261 : perda 0.07051\n",
      "Época 262 : perda 0.06864\n",
      "Época 263 : perda 0.06410\n",
      "Época 264 : perda 0.07374\n",
      "Época 265 : perda 0.08896\n",
      "Época 266 : perda 0.07557\n",
      "Época 267 : perda 0.06614\n",
      "Época 268 : perda 0.09202\n",
      "Época 269 : perda 0.07077\n",
      "Época 270 : perda 0.08587\n",
      "Época 271 : perda 0.09467\n",
      "Época 272 : perda 0.07363\n",
      "Época 273 : perda 0.12420\n",
      "Época 274 : perda 0.11120\n",
      "Época 275 : perda 0.14531\n",
      "Época 276 : perda 0.13567\n",
      "Época 277 : perda 0.07394\n",
      "Época 278 : perda 0.06316\n",
      "Época 279 : perda 0.06900\n",
      "Época 280 : perda 0.06000\n",
      "Época 281 : perda 0.05659\n",
      "Época 282 : perda 0.06311\n",
      "Época 283 : perda 0.07193\n",
      "Época 284 : perda 0.06762\n",
      "Época 285 : perda 0.08498\n",
      "Época 286 : perda 0.06105\n",
      "Época 287 : perda 0.06021\n",
      "Época 288 : perda 0.06685\n",
      "Época 289 : perda 0.07059\n",
      "Época 290 : perda 0.05111\n",
      "Época 291 : perda 0.10319\n",
      "Época 292 : perda 0.09082\n",
      "Época 293 : perda 0.09877\n",
      "Época 294 : perda 0.05746\n",
      "Época 295 : perda 0.06582\n",
      "Época 296 : perda 0.08330\n",
      "Época 297 : perda 0.07215\n",
      "Época 298 : perda 0.12137\n",
      "Época 299 : perda 0.10373\n",
      "Época 300 : perda 0.06728\n",
      "Época 301 : perda 0.06831\n",
      "Época 302 : perda 0.07794\n",
      "Época 303 : perda 0.12363\n",
      "Época 304 : perda 0.08022\n",
      "Época 305 : perda 0.06270\n",
      "Época 306 : perda 0.05905\n",
      "Época 307 : perda 0.09556\n",
      "Época 308 : perda 0.07362\n",
      "Época 309 : perda 0.06807\n",
      "Época 310 : perda 0.06271\n",
      "Época 311 : perda 0.07109\n",
      "Época 312 : perda 0.06218\n",
      "Época 313 : perda 0.05452\n",
      "Época 314 : perda 0.06573\n",
      "Época 315 : perda 0.05331\n",
      "Época 316 : perda 0.06406\n",
      "Época 317 : perda 0.07405\n",
      "Época 318 : perda 0.06942\n",
      "Época 319 : perda 0.06750\n",
      "Época 320 : perda 0.06111\n",
      "Época 321 : perda 0.05376\n",
      "Época 322 : perda 0.05158\n",
      "Época 323 : perda 0.06970\n",
      "Época 324 : perda 0.05866\n",
      "Época 325 : perda 0.09069\n",
      "Época 326 : perda 0.10103\n",
      "Época 327 : perda 0.10188\n",
      "Época 328 : perda 0.09210\n",
      "Época 329 : perda 0.10368\n",
      "Época 330 : perda 0.06420\n",
      "Época 331 : perda 0.06254\n",
      "Época 332 : perda 0.05353\n",
      "Época 333 : perda 0.13832\n",
      "Época 334 : perda 0.09893\n",
      "Época 335 : perda 0.06020\n",
      "Época 336 : perda 0.08387\n",
      "Época 337 : perda 0.06745\n",
      "Época 338 : perda 0.08517\n",
      "Época 339 : perda 0.14222\n",
      "Época 340 : perda 0.14255\n",
      "Época 341 : perda 0.09547\n",
      "Época 342 : perda 0.06789\n",
      "Época 343 : perda 0.07263\n",
      "Época 344 : perda 0.05649\n",
      "Época 345 : perda 0.05740\n",
      "Época 346 : perda 0.05221\n",
      "Época 347 : perda 0.04778\n",
      "Época 348 : perda 0.08035\n",
      "Época 349 : perda 0.05198\n",
      "Época 350 : perda 0.06436\n",
      "Época 351 : perda 0.05146\n",
      "Época 352 : perda 0.05359\n",
      "Época 353 : perda 0.09604\n",
      "Época 354 : perda 0.07120\n",
      "Época 355 : perda 0.05078\n",
      "Época 356 : perda 0.05060\n",
      "Época 357 : perda 0.08186\n",
      "Época 358 : perda 0.05478\n",
      "Época 359 : perda 0.06429\n",
      "Época 360 : perda 0.06687\n",
      "Época 361 : perda 0.08770\n",
      "Época 362 : perda 0.04630\n",
      "Época 363 : perda 0.05782\n",
      "Época 364 : perda 0.05047\n",
      "Época 365 : perda 0.06564\n",
      "Época 366 : perda 0.07034\n",
      "Época 367 : perda 0.10630\n",
      "Época 368 : perda 0.08568\n",
      "Época 369 : perda 0.12515\n",
      "Época 370 : perda 0.06803\n",
      "Época 371 : perda 0.04722\n",
      "Época 372 : perda 0.05495\n",
      "Época 373 : perda 0.04633\n",
      "Época 374 : perda 0.04765\n",
      "Época 375 : perda 0.06198\n",
      "Época 376 : perda 0.15288\n",
      "Época 377 : perda 0.06878\n",
      "Época 378 : perda 0.06825\n",
      "Época 379 : perda 0.09560\n",
      "Época 380 : perda 0.09314\n",
      "Época 381 : perda 0.05557\n",
      "Época 382 : perda 0.05370\n",
      "Época 383 : perda 0.05964\n",
      "Época 384 : perda 0.05100\n",
      "Época 385 : perda 0.12801\n",
      "Época 386 : perda 0.09815\n",
      "Época 387 : perda 0.06461\n",
      "Época 388 : perda 0.05172\n",
      "Época 389 : perda 0.06017\n",
      "Época 390 : perda 0.05685\n",
      "Época 391 : perda 0.09940\n",
      "Época 392 : perda 0.08468\n",
      "Época 393 : perda 0.06565\n",
      "Época 394 : perda 0.06643\n",
      "Época 395 : perda 0.04501\n",
      "Época 396 : perda 0.05092\n",
      "Época 397 : perda 0.11724\n",
      "Época 398 : perda 0.07920\n",
      "Época 399 : perda 0.07427\n",
      "Época 400 : perda 0.04207\n",
      "Época 401 : perda 0.08441\n",
      "Época 402 : perda 0.18121\n",
      "Época 403 : perda 0.07289\n",
      "Época 404 : perda 0.05616\n",
      "Época 405 : perda 0.05022\n",
      "Época 406 : perda 0.04776\n",
      "Época 407 : perda 0.04280\n",
      "Época 408 : perda 0.04738\n",
      "Época 409 : perda 0.09630\n",
      "Época 410 : perda 0.06501\n",
      "Época 411 : perda 0.05870\n",
      "Época 412 : perda 0.08458\n",
      "Época 413 : perda 0.04561\n",
      "Época 414 : perda 0.04334\n",
      "Época 415 : perda 0.05706\n",
      "Época 416 : perda 0.05864\n",
      "Época 417 : perda 0.06654\n",
      "Época 418 : perda 0.05303\n",
      "Época 419 : perda 0.04374\n",
      "Época 420 : perda 0.04586\n",
      "Época 421 : perda 0.08474\n",
      "Época 422 : perda 0.05067\n",
      "Época 423 : perda 0.03988\n",
      "Época 424 : perda 0.04233\n",
      "Época 425 : perda 0.04199\n",
      "Época 426 : perda 0.05867\n",
      "Época 427 : perda 0.05100\n",
      "Época 428 : perda 0.04599\n",
      "Época 429 : perda 0.05018\n",
      "Época 430 : perda 0.04118\n",
      "Época 431 : perda 0.04339\n",
      "Época 432 : perda 0.04325\n",
      "Época 433 : perda 0.05605\n",
      "Época 434 : perda 0.04829\n",
      "Época 435 : perda 0.06206\n",
      "Época 436 : perda 0.04981\n",
      "Época 437 : perda 0.05592\n",
      "Época 438 : perda 0.04630\n",
      "Época 439 : perda 0.08537\n",
      "Época 440 : perda 0.09735\n",
      "Época 441 : perda 0.04683\n",
      "Época 442 : perda 0.04816\n",
      "Época 443 : perda 0.04854\n",
      "Época 444 : perda 0.13092\n",
      "Época 445 : perda 0.15540\n",
      "Época 446 : perda 0.06676\n",
      "Época 447 : perda 0.04793\n",
      "Época 448 : perda 0.05851\n",
      "Época 449 : perda 0.06680\n",
      "Época 450 : perda 0.06850\n",
      "Época 451 : perda 0.05684\n",
      "Época 452 : perda 0.04206\n",
      "Época 453 : perda 0.11399\n",
      "Época 454 : perda 0.05957\n",
      "Época 455 : perda 0.04479\n",
      "Época 456 : perda 0.04017\n",
      "Época 457 : perda 0.04727\n",
      "Época 458 : perda 0.06464\n",
      "Época 459 : perda 0.07864\n",
      "Época 460 : perda 0.06919\n",
      "Época 461 : perda 0.04644\n",
      "Época 462 : perda 0.09167\n",
      "Época 463 : perda 0.05479\n",
      "Época 464 : perda 0.06701\n",
      "Época 465 : perda 0.05562\n",
      "Época 466 : perda 0.05001\n",
      "Época 467 : perda 0.04042\n",
      "Época 468 : perda 0.04122\n",
      "Época 469 : perda 0.06620\n",
      "Época 470 : perda 0.04427\n",
      "Época 471 : perda 0.06121\n",
      "Época 472 : perda 0.04116\n",
      "Época 473 : perda 0.04793\n",
      "Época 474 : perda 0.06113\n",
      "Época 475 : perda 0.04658\n",
      "Época 476 : perda 0.06217\n",
      "Época 477 : perda 0.05334\n",
      "Época 478 : perda 0.06501\n",
      "Época 479 : perda 0.04643\n",
      "Época 480 : perda 0.06018\n",
      "Época 481 : perda 0.04976\n",
      "Época 482 : perda 0.07913\n",
      "Época 483 : perda 0.14947\n",
      "Época 484 : perda 0.06684\n",
      "Época 485 : perda 0.04891\n",
      "Época 486 : perda 0.05813\n",
      "Época 487 : perda 0.24271\n",
      "Época 488 : perda 0.06599\n",
      "Época 489 : perda 0.05908\n",
      "Época 490 : perda 0.04485\n",
      "Época 491 : perda 0.04356\n",
      "Época 492 : perda 0.05342\n",
      "Época 493 : perda 0.06274\n",
      "Época 494 : perda 0.04562\n",
      "Época 495 : perda 0.05180\n",
      "Época 496 : perda 0.04696\n",
      "Época 497 : perda 0.04731\n",
      "Época 498 : perda 0.05459\n",
      "Época 499 : perda 0.05206\n",
      "Época 500 : perda 0.07515\n",
      "Época 501 : perda 0.11189\n",
      "Época 502 : perda 0.07201\n",
      "Época 503 : perda 0.05401\n",
      "Época 504 : perda 0.07530\n",
      "Época 505 : perda 0.05848\n",
      "Época 506 : perda 0.04661\n",
      "Época 507 : perda 0.04859\n",
      "Época 508 : perda 0.04410\n",
      "Época 509 : perda 0.04917\n",
      "Época 510 : perda 0.06397\n",
      "Época 511 : perda 0.07463\n",
      "Época 512 : perda 0.05704\n",
      "Época 513 : perda 0.06843\n",
      "Época 514 : perda 0.05609\n",
      "Época 515 : perda 0.04049\n",
      "Época 516 : perda 0.03857\n",
      "Época 517 : perda 0.03529\n",
      "Época 518 : perda 0.04238\n",
      "Época 519 : perda 0.04399\n",
      "Época 520 : perda 0.07088\n",
      "Época 521 : perda 0.14035\n",
      "Época 522 : perda 0.09791\n",
      "Época 523 : perda 0.05600\n",
      "Época 524 : perda 0.05694\n",
      "Época 525 : perda 0.05987\n",
      "Época 526 : perda 0.08384\n",
      "Época 527 : perda 0.08882\n",
      "Época 528 : perda 0.06496\n",
      "Época 529 : perda 0.11167\n",
      "Época 530 : perda 0.06359\n",
      "Época 531 : perda 0.04062\n",
      "Época 532 : perda 0.03750\n",
      "Época 533 : perda 0.04530\n",
      "Época 534 : perda 0.05606\n",
      "Época 535 : perda 0.05080\n",
      "Época 536 : perda 0.04219\n",
      "Época 537 : perda 0.04475\n",
      "Época 538 : perda 0.03606\n",
      "Época 539 : perda 0.03989\n",
      "Época 540 : perda 0.03343\n",
      "Época 541 : perda 0.03536\n",
      "Época 542 : perda 0.03926\n",
      "Época 543 : perda 0.04014\n",
      "Época 544 : perda 0.05809\n",
      "Época 545 : perda 0.05410\n",
      "Época 546 : perda 0.05060\n",
      "Época 547 : perda 0.04830\n",
      "Época 548 : perda 0.03462\n",
      "Época 549 : perda 0.04507\n",
      "Época 550 : perda 0.14450\n",
      "Época 551 : perda 0.17268\n",
      "Época 552 : perda 0.11517\n",
      "Época 553 : perda 0.07659\n",
      "Época 554 : perda 0.04906\n",
      "Época 555 : perda 0.06469\n",
      "Época 556 : perda 0.04717\n",
      "Época 557 : perda 0.05221\n",
      "Época 558 : perda 0.04404\n",
      "Época 559 : perda 0.04952\n",
      "Época 560 : perda 0.04116\n",
      "Época 561 : perda 0.03952\n",
      "Época 562 : perda 0.04227\n",
      "Época 563 : perda 0.04687\n",
      "Época 564 : perda 0.08013\n",
      "Época 565 : perda 0.08180\n",
      "Época 566 : perda 0.04842\n",
      "Época 567 : perda 0.03905\n",
      "Época 568 : perda 0.04016\n",
      "Época 569 : perda 0.03155\n",
      "Época 570 : perda 0.04834\n",
      "Época 571 : perda 0.05601\n",
      "Época 572 : perda 0.04143\n",
      "Época 573 : perda 0.04986\n",
      "Época 574 : perda 0.04117\n",
      "Época 575 : perda 0.03416\n",
      "Época 576 : perda 0.03179\n",
      "Época 577 : perda 0.04294\n",
      "Época 578 : perda 0.03888\n",
      "Época 579 : perda 0.03344\n",
      "Época 580 : perda 0.03710\n",
      "Época 581 : perda 0.03861\n",
      "Época 582 : perda 0.03922\n",
      "Época 583 : perda 0.03197\n",
      "Época 584 : perda 0.03456\n",
      "Época 585 : perda 0.04072\n",
      "Época 586 : perda 0.05015\n",
      "Época 587 : perda 0.08933\n",
      "Época 588 : perda 0.09268\n",
      "Época 589 : perda 0.07794\n",
      "Época 590 : perda 0.42656\n",
      "Época 591 : perda 0.12348\n",
      "Época 592 : perda 0.04536\n",
      "Época 593 : perda 0.04451\n",
      "Época 594 : perda 0.03996\n",
      "Época 595 : perda 0.04178\n",
      "Época 596 : perda 0.03931\n",
      "Época 597 : perda 0.03910\n",
      "Época 598 : perda 0.06165\n",
      "Época 599 : perda 0.06351\n",
      "Época 600 : perda 0.05268\n",
      "Época 601 : perda 0.03880\n",
      "Época 602 : perda 0.03266\n",
      "Época 603 : perda 0.02863\n",
      "Época 604 : perda 0.03465\n",
      "Época 605 : perda 0.04119\n",
      "Época 606 : perda 0.06265\n",
      "Época 607 : perda 0.04780\n",
      "Época 608 : perda 0.06534\n",
      "Época 609 : perda 0.07471\n",
      "Época 610 : perda 0.07843\n",
      "Época 611 : perda 0.07835\n",
      "Época 612 : perda 0.07466\n",
      "Época 613 : perda 0.06484\n",
      "Época 614 : perda 0.04668\n",
      "Época 615 : perda 0.04602\n",
      "Época 616 : perda 0.03872\n",
      "Época 617 : perda 0.04195\n",
      "Época 618 : perda 0.03705\n",
      "Época 619 : perda 0.03379\n",
      "Época 620 : perda 0.07328\n",
      "Época 621 : perda 0.15717\n",
      "Época 622 : perda 0.04783\n",
      "Época 623 : perda 0.03447\n",
      "Época 624 : perda 0.04362\n",
      "Época 625 : perda 0.02973\n",
      "Época 626 : perda 0.03898\n",
      "Época 627 : perda 0.03214\n",
      "Época 628 : perda 0.03448\n",
      "Época 629 : perda 0.03226\n",
      "Época 630 : perda 0.07571\n",
      "Época 631 : perda 0.07519\n",
      "Época 632 : perda 0.03571\n",
      "Época 633 : perda 0.02966\n",
      "Época 634 : perda 0.03977\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Época 635 : perda 0.05955\n",
      "Época 636 : perda 0.03209\n",
      "Época 637 : perda 0.02919\n",
      "Época 638 : perda 0.03463\n",
      "Época 639 : perda 0.03352\n",
      "Época 640 : perda 0.05375\n",
      "Época 641 : perda 0.13944\n",
      "Época 642 : perda 0.17011\n",
      "Época 643 : perda 0.05558\n",
      "Época 644 : perda 0.04036\n",
      "Época 645 : perda 0.03414\n",
      "Época 646 : perda 0.03544\n",
      "Época 647 : perda 0.03520\n",
      "Época 648 : perda 0.03247\n",
      "Época 649 : perda 0.03382\n",
      "Época 650 : perda 0.03266\n",
      "Época 651 : perda 0.03290\n",
      "Época 652 : perda 0.03342\n",
      "Época 653 : perda 0.03124\n",
      "Época 654 : perda 0.03061\n",
      "Época 655 : perda 0.03014\n",
      "Época 656 : perda 0.02807\n",
      "Época 657 : perda 0.03694\n",
      "Época 658 : perda 0.03106\n",
      "Época 659 : perda 0.03415\n",
      "Época 660 : perda 0.04069\n",
      "Época 661 : perda 0.07766\n",
      "Época 662 : perda 0.09636\n",
      "Época 663 : perda 0.14872\n",
      "Época 664 : perda 0.33555\n",
      "Época 665 : perda 0.28526\n",
      "Época 666 : perda 0.27529\n",
      "Época 667 : perda 0.16082\n",
      "Época 668 : perda 0.05451\n",
      "Época 669 : perda 0.28387\n",
      "Época 670 : perda 0.04356\n",
      "Época 671 : perda 0.03397\n",
      "Época 672 : perda 0.03868\n",
      "Época 673 : perda 0.03277\n",
      "Época 674 : perda 0.03662\n",
      "Época 675 : perda 0.03714\n",
      "Época 676 : perda 0.04027\n",
      "Época 677 : perda 0.03307\n",
      "Época 678 : perda 0.03783\n",
      "Época 679 : perda 0.03773\n",
      "Época 680 : perda 0.04147\n",
      "Época 681 : perda 0.05032\n",
      "Época 682 : perda 0.03661\n",
      "Época 683 : perda 0.03037\n",
      "Época 684 : perda 0.03356\n",
      "Época 685 : perda 0.02874\n",
      "Época 686 : perda 0.02790\n",
      "Época 687 : perda 0.03928\n",
      "Época 688 : perda 0.03091\n",
      "Época 689 : perda 0.07003\n",
      "Época 690 : perda 0.09818\n",
      "Época 691 : perda 0.06033\n",
      "Época 692 : perda 0.03799\n",
      "Época 693 : perda 0.07965\n",
      "Época 694 : perda 0.03227\n",
      "Época 695 : perda 0.03126\n",
      "Época 696 : perda 0.03844\n",
      "Época 697 : perda 0.05112\n",
      "Época 698 : perda 0.03205\n",
      "Época 699 : perda 0.02595\n",
      "Época 700 : perda 0.03335\n",
      "Época 701 : perda 0.02924\n",
      "Época 702 : perda 0.03151\n",
      "Época 703 : perda 0.02832\n",
      "Época 704 : perda 0.02961\n",
      "Época 705 : perda 0.03234\n",
      "Época 706 : perda 0.03102\n",
      "Época 707 : perda 0.06733\n",
      "Época 708 : perda 0.03734\n",
      "Época 709 : perda 0.04135\n",
      "Época 710 : perda 0.05124\n",
      "Época 711 : perda 0.04105\n",
      "Época 712 : perda 0.05166\n",
      "Época 713 : perda 0.03410\n",
      "Época 714 : perda 0.03997\n",
      "Época 715 : perda 0.03401\n",
      "Época 716 : perda 0.03091\n",
      "Época 717 : perda 0.02802\n",
      "Época 718 : perda 0.03155\n",
      "Época 719 : perda 0.03986\n",
      "Época 720 : perda 0.04222\n",
      "Época 721 : perda 0.03973\n",
      "Época 722 : perda 0.03488\n",
      "Época 723 : perda 0.06153\n",
      "Época 724 : perda 0.05705\n",
      "Época 725 : perda 0.08166\n",
      "Época 726 : perda 0.06347\n",
      "Época 727 : perda 0.04463\n",
      "Época 728 : perda 0.04266\n",
      "Época 729 : perda 0.03984\n",
      "Época 730 : perda 0.03056\n",
      "Época 731 : perda 0.06146\n",
      "Época 732 : perda 0.03611\n",
      "Época 733 : perda 0.05892\n",
      "Época 734 : perda 0.06288\n",
      "Época 735 : perda 0.03803\n",
      "Época 736 : perda 0.02951\n",
      "Época 737 : perda 0.03043\n",
      "Época 738 : perda 0.02495\n",
      "Época 739 : perda 0.02764\n",
      "Época 740 : perda 0.02742\n",
      "Época 741 : perda 0.02883\n",
      "Época 742 : perda 0.03144\n",
      "Época 743 : perda 0.02366\n",
      "Época 744 : perda 0.03398\n",
      "Época 745 : perda 0.03698\n",
      "Época 746 : perda 0.02784\n",
      "Época 747 : perda 0.03457\n",
      "Época 748 : perda 0.03732\n",
      "Época 749 : perda 0.03683\n",
      "Época 750 : perda 0.07959\n",
      "Época 751 : perda 0.07661\n",
      "Época 752 : perda 0.05511\n",
      "Época 753 : perda 0.03515\n",
      "Época 754 : perda 0.03595\n",
      "Época 755 : perda 0.05103\n",
      "Época 756 : perda 0.03446\n",
      "Época 757 : perda 0.03209\n",
      "Época 758 : perda 0.03681\n",
      "Época 759 : perda 0.03158\n",
      "Época 760 : perda 0.03088\n",
      "Época 761 : perda 0.03339\n",
      "Época 762 : perda 0.02994\n",
      "Época 763 : perda 0.03215\n",
      "Época 764 : perda 0.04237\n",
      "Época 765 : perda 0.04828\n",
      "Época 766 : perda 0.07784\n",
      "Época 767 : perda 0.09053\n",
      "Época 768 : perda 0.04670\n",
      "Época 769 : perda 0.06546\n",
      "Época 770 : perda 0.12284\n",
      "Época 771 : perda 0.04551\n",
      "Época 772 : perda 0.04400\n",
      "Época 773 : perda 0.03134\n",
      "Época 774 : perda 0.05556\n",
      "Época 775 : perda 0.04156\n",
      "Época 776 : perda 0.02968\n",
      "Época 777 : perda 0.02460\n",
      "Época 778 : perda 0.02981\n",
      "Época 779 : perda 0.02688\n",
      "Época 780 : perda 0.02571\n",
      "Época 781 : perda 0.02682\n",
      "Época 782 : perda 0.02649\n",
      "Época 783 : perda 0.04726\n",
      "Época 784 : perda 0.04332\n",
      "Época 785 : perda 0.05662\n",
      "Época 786 : perda 0.08194\n",
      "Época 787 : perda 0.09691\n",
      "Época 788 : perda 0.06589\n",
      "Época 789 : perda 0.06339\n",
      "Época 790 : perda 0.05728\n",
      "Época 791 : perda 0.03572\n",
      "Época 792 : perda 0.03027\n",
      "Época 793 : perda 0.02750\n",
      "Época 794 : perda 0.03240\n",
      "Época 795 : perda 0.03944\n",
      "Época 796 : perda 0.04666\n",
      "Época 797 : perda 0.02632\n",
      "Época 798 : perda 0.03149\n",
      "Época 799 : perda 0.02999\n",
      "Época 800 : perda 0.03601\n",
      "Época 801 : perda 0.07866\n",
      "Época 802 : perda 0.08071\n",
      "Época 803 : perda 0.04446\n",
      "Época 804 : perda 0.02984\n",
      "Época 805 : perda 0.03702\n",
      "Época 806 : perda 0.04494\n",
      "Época 807 : perda 0.03594\n",
      "Época 808 : perda 0.02754\n",
      "Época 809 : perda 0.02383\n",
      "Época 810 : perda 0.04219\n",
      "Época 811 : perda 0.05711\n",
      "Época 812 : perda 0.30447\n",
      "Época 813 : perda 0.18529\n",
      "Época 814 : perda 0.06160\n",
      "Época 815 : perda 0.03501\n",
      "Época 816 : perda 0.02766\n",
      "Época 817 : perda 0.03028\n",
      "Época 818 : perda 0.03137\n",
      "Época 819 : perda 0.02896\n",
      "Época 820 : perda 0.02729\n",
      "Época 821 : perda 0.03077\n",
      "Época 822 : perda 0.03529\n",
      "Época 823 : perda 0.02707\n",
      "Época 824 : perda 0.02540\n",
      "Época 825 : perda 0.02542\n",
      "Época 826 : perda 0.02398\n",
      "Época 827 : perda 0.02807\n",
      "Época 828 : perda 0.02568\n",
      "Época 829 : perda 0.02453\n",
      "Época 830 : perda 0.02430\n",
      "Época 831 : perda 0.02360\n",
      "Época 832 : perda 0.02419\n",
      "Época 833 : perda 0.02611\n",
      "Época 834 : perda 0.02482\n",
      "Época 835 : perda 0.02767\n",
      "Época 836 : perda 0.02352\n",
      "Época 837 : perda 0.02833\n",
      "Época 838 : perda 0.02827\n",
      "Época 839 : perda 0.02553\n",
      "Época 840 : perda 0.02892\n",
      "Época 841 : perda 0.02559\n",
      "Época 842 : perda 0.03648\n",
      "Época 843 : perda 0.06391\n",
      "Época 844 : perda 0.02856\n",
      "Época 845 : perda 0.06822\n",
      "Época 846 : perda 0.04317\n",
      "Época 847 : perda 0.31290\n",
      "Época 848 : perda 0.17179\n",
      "Época 849 : perda 0.07757\n",
      "Época 850 : perda 0.16169\n",
      "Época 851 : perda 0.27674\n",
      "Época 852 : perda 0.04819\n",
      "Época 853 : perda 0.09142\n",
      "Época 854 : perda 0.03985\n",
      "Época 855 : perda 0.02898\n",
      "Época 856 : perda 0.02834\n",
      "Época 857 : perda 0.02717\n",
      "Época 858 : perda 0.02608\n",
      "Época 859 : perda 0.02440\n",
      "Época 860 : perda 0.02487\n",
      "Época 861 : perda 0.02417\n",
      "Época 862 : perda 0.02709\n",
      "Época 863 : perda 0.02644\n",
      "Época 864 : perda 0.03851\n",
      "Época 865 : perda 0.02868\n",
      "Época 866 : perda 0.02368\n",
      "Época 867 : perda 0.02813\n",
      "Época 868 : perda 0.02401\n",
      "Época 869 : perda 0.02786\n",
      "Época 870 : perda 0.02464\n",
      "Época 871 : perda 0.02513\n",
      "Época 872 : perda 0.02230\n",
      "Época 873 : perda 0.02357\n",
      "Época 874 : perda 0.02344\n",
      "Época 875 : perda 0.02330\n",
      "Época 876 : perda 0.03349\n",
      "Época 877 : perda 0.03752\n",
      "Época 878 : perda 0.06481\n",
      "Época 879 : perda 0.02851\n",
      "Época 880 : perda 0.02834\n",
      "Época 881 : perda 0.02602\n",
      "Época 882 : perda 0.02391\n",
      "Época 883 : perda 0.02208\n",
      "Época 884 : perda 0.02259\n",
      "Época 885 : perda 0.02326\n",
      "Época 886 : perda 0.03227\n",
      "Época 887 : perda 0.04841\n",
      "Época 888 : perda 0.14041\n",
      "Época 889 : perda 0.10734\n",
      "Época 890 : perda 0.09344\n",
      "Época 891 : perda 0.55547\n",
      "Época 892 : perda 0.11665\n",
      "Época 893 : perda 0.06311\n",
      "Época 894 : perda 0.03805\n",
      "Época 895 : perda 0.03062\n",
      "Época 896 : perda 0.02952\n",
      "Época 897 : perda 0.02844\n",
      "Época 898 : perda 0.02690\n",
      "Época 899 : perda 0.02624\n",
      "Época 900 : perda 0.02849\n",
      "Época 901 : perda 0.03736\n",
      "Época 902 : perda 0.02899\n",
      "Época 903 : perda 0.02946\n",
      "Época 904 : perda 0.03228\n",
      "Época 905 : perda 0.03308\n",
      "Época 906 : perda 0.02642\n",
      "Época 907 : perda 0.02498\n",
      "Época 908 : perda 0.02420\n",
      "Época 909 : perda 0.03302\n",
      "Época 910 : perda 0.02472\n",
      "Época 911 : perda 0.02680\n",
      "Época 912 : perda 0.02608\n",
      "Época 913 : perda 0.04617\n",
      "Época 914 : perda 0.03048\n",
      "Época 915 : perda 0.02284\n",
      "Época 916 : perda 0.02223\n",
      "Época 917 : perda 0.03462\n",
      "Época 918 : perda 0.02987\n",
      "Época 919 : perda 0.02350\n",
      "Época 920 : perda 0.02341\n",
      "Época 921 : perda 0.02273\n",
      "Época 922 : perda 0.02504\n",
      "Época 923 : perda 0.03077\n",
      "Época 924 : perda 0.04024\n",
      "Época 925 : perda 0.09610\n",
      "Época 926 : perda 0.06026\n",
      "Época 927 : perda 0.08706\n",
      "Época 928 : perda 0.12422\n",
      "Época 929 : perda 0.08790\n",
      "Época 930 : perda 0.29131\n",
      "Época 931 : perda 0.10420\n",
      "Época 932 : perda 0.03581\n",
      "Época 933 : perda 0.02681\n",
      "Época 934 : perda 0.02861\n",
      "Época 935 : perda 0.02627\n",
      "Época 936 : perda 0.02794\n",
      "Época 937 : perda 0.02386\n",
      "Época 938 : perda 0.02276\n",
      "Época 939 : perda 0.03123\n",
      "Época 940 : perda 0.02386\n",
      "Época 941 : perda 0.02099\n",
      "Época 942 : perda 0.02626\n",
      "Época 943 : perda 0.02660\n",
      "Época 944 : perda 0.02497\n",
      "Época 945 : perda 0.02933\n",
      "Época 946 : perda 0.08577\n",
      "Época 947 : perda 0.04481\n",
      "Época 948 : perda 0.02538\n",
      "Época 949 : perda 0.02729\n",
      "Época 950 : perda 0.02859\n",
      "Época 951 : perda 0.06252\n",
      "Época 952 : perda 0.11886\n",
      "Época 953 : perda 0.03558\n",
      "Época 954 : perda 0.02552\n",
      "Época 955 : perda 0.02454\n",
      "Época 956 : perda 0.02338\n",
      "Época 957 : perda 0.02375\n",
      "Época 958 : perda 0.02579\n",
      "Época 959 : perda 0.02492\n",
      "Época 960 : perda 0.02368\n",
      "Época 961 : perda 0.02945\n",
      "Época 962 : perda 0.02539\n",
      "Época 963 : perda 0.02805\n",
      "Época 964 : perda 0.02688\n",
      "Época 965 : perda 0.02187\n",
      "Época 966 : perda 0.02197\n",
      "Época 967 : perda 0.02199\n",
      "Época 968 : perda 0.02646\n",
      "Época 969 : perda 0.02502\n",
      "Época 970 : perda 0.09706\n",
      "Época 971 : perda 0.05155\n",
      "Época 972 : perda 0.03748\n",
      "Época 973 : perda 0.02197\n",
      "Época 974 : perda 0.02163\n",
      "Época 975 : perda 0.02207\n",
      "Época 976 : perda 0.02627\n",
      "Época 977 : perda 0.03293\n",
      "Época 978 : perda 0.03703\n",
      "Época 979 : perda 0.02423\n",
      "Época 980 : perda 0.02345\n",
      "Época 981 : perda 0.02088\n",
      "Época 982 : perda 0.02250\n",
      "Época 983 : perda 0.03480\n",
      "Época 984 : perda 0.02784\n",
      "Época 985 : perda 0.06322\n",
      "Época 986 : perda 0.22995\n",
      "Época 987 : perda 0.09124\n",
      "Época 988 : perda 0.09130\n",
      "Época 989 : perda 0.10730\n",
      "Época 990 : perda 0.05253\n",
      "Época 991 : perda 0.03314\n",
      "Época 992 : perda 0.04063\n",
      "Época 993 : perda 0.03601\n",
      "Época 994 : perda 0.04099\n",
      "Época 995 : perda 0.02725\n",
      "Época 996 : perda 0.02295\n",
      "Época 997 : perda 0.02861\n",
      "Época 998 : perda 0.03418\n",
      "Época 999 : perda 0.02502\n",
      "Época 1000 : perda 0.03052\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1000):\n",
    "    sum_loss = 0\n",
    "    for data in train_loader:\n",
    "        xi,yi = data\n",
    "        optimizer.zero_grad()\n",
    "        saidas = model.forward(xi)\n",
    "        loss = criterion(saidas,yi)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        sum_loss += loss.item()\n",
    "    print('Época %d : perda %.5f'%(epoch+1, sum_loss/len(train_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fsf7cCwY7ipb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GjFgVCqVibgp"
   },
   "source": [
    "## Etapa 6: Visualização dos pesos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 837,
     "status": "ok",
     "timestamp": 1595025619997,
     "user": {
      "displayName": "Matheus Pereira de Novaes",
      "photoUrl": "",
      "userId": "01574286574804862444"
     },
     "user_tz": 180
    },
    "id": "bp-fi9sV7jwl",
    "outputId": "bf36574d-7a13-4147-ccfc-e018d26c90f4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 30])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = list(model.parameters())\n",
    "pesos0 = params[0]\n",
    "pesos0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1680,
     "status": "ok",
     "timestamp": 1595025658468,
     "user": {
      "displayName": "Matheus Pereira de Novaes",
      "photoUrl": "",
      "userId": "01574286574804862444"
     },
     "user_tz": 180
    },
    "id": "hAZ4mQNw7-AT",
    "outputId": "ce0626ce-12e3-40dc-ed3c-a784c7b30b0d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-3.8546e-01, -6.8412e-01, -3.7806e-01,  3.4219e-02,  9.6729e-02,\n",
      "          1.8673e-02,  9.2617e-02,  1.9712e-02, -2.2701e-02,  4.5489e-01,\n",
      "          5.1713e-02, -1.8309e-02, -5.1198e-02,  1.7017e-01,  9.3667e-05,\n",
      "         -4.3663e-02,  3.9667e-02,  1.4397e-03,  6.8141e-01,  2.5912e-02,\n",
      "         -2.7203e-01, -7.0616e-01, -2.2815e-01,  1.3665e-01, -1.9458e-02,\n",
      "          1.5015e-01, -2.8939e-01,  6.0553e-02, -3.8405e-02, -6.9967e-03],\n",
      "        [ 7.1046e-01, -2.0617e-03,  7.1852e-01,  1.9343e-01, -5.0275e-01,\n",
      "          6.3833e-01, -5.5340e-01,  9.7418e-02,  3.9100e-02, -5.0183e-01,\n",
      "         -2.1290e-01, -6.1585e-02, -4.7095e-02, -5.5592e-02, -1.8442e-03,\n",
      "          2.7916e-02,  1.4743e-01, -2.0803e-02, -7.6303e-01, -7.1539e-03,\n",
      "          7.3212e-01, -4.1245e-01,  4.1412e-01, -4.5000e-02, -1.0329e-01,\n",
      "          1.0071e-01, -6.5133e-01,  1.7939e-01, -6.2097e-01, -1.1543e-01],\n",
      "        [-2.2785e-11, -1.3317e-10, -2.9893e-03, -2.1758e-02, -1.3459e-18,\n",
      "          1.8568e-19, -5.5699e-19,  1.3678e-19, -7.9382e-19,  1.1149e-18,\n",
      "         -7.7049e-19,  2.1479e-18,  5.5528e-18, -1.8132e-07, -1.4858e-19,\n",
      "         -5.9869e-19,  3.9332e-19,  2.0579e-19, -2.8945e-19,  6.7067e-20,\n",
      "         -4.7949e-10, -6.2283e-08, -4.1566e-03, -2.4333e-02, -7.9301e-19,\n",
      "          2.4129e-18, -1.8017e-19, -1.9125e-18,  2.1654e-18,  3.2663e-19],\n",
      "        [-8.0684e-01, -2.4063e-01, -6.0584e-01,  1.5615e-01, -5.9672e-02,\n",
      "          8.7303e-02,  5.2823e-02,  4.0611e-03, -2.1116e-01, -1.5423e-01,\n",
      "          1.0875e-01, -8.8017e-02,  1.6304e-02,  6.7087e-01,  1.6123e-03,\n",
      "          1.4957e-02,  1.4026e-02,  3.2490e-03,  5.4426e-03,  1.4095e-03,\n",
      "         -5.4537e-01, -3.9320e-01, -5.3731e-01,  8.3455e-02,  2.8369e-01,\n",
      "         -6.7057e-02, -1.4114e-01,  1.7185e-01,  1.0727e-01,  2.4547e-01],\n",
      "        [-9.2292e-01,  5.8168e-01, -4.2970e-01, -1.5700e-01,  5.9105e-01,\n",
      "          6.1449e-01, -1.4898e-01, -1.6330e-01, -5.4891e-01,  5.5347e-01,\n",
      "         -1.8596e-01, -1.7610e-02, -2.6733e-02,  1.0021e-01,  6.3889e-03,\n",
      "          1.2138e-02, -1.8232e-01,  2.8455e-02,  8.9693e-01,  4.6476e-02,\n",
      "         -6.4351e-01,  1.2675e+00, -2.2011e-01,  3.7995e-01, -3.6318e-01,\n",
      "         -2.6047e-01, -7.2177e-01, -2.0843e-01, -1.9896e-01,  7.0370e-01],\n",
      "        [ 7.5502e-03,  1.3508e-02,  5.2595e-03, -3.9440e-02, -1.3213e-03,\n",
      "          7.9983e-11,  1.1024e-02, -1.6108e-11,  6.5635e-07, -1.4313e-04,\n",
      "         -4.6476e-04,  3.5858e-02, -8.3177e-03,  1.9572e-01,  3.1039e-12,\n",
      "         -5.7283e-13, -3.3765e-12,  1.5302e-12,  1.3768e-11, -1.2096e-12,\n",
      "          6.4874e-03,  1.5552e-02,  1.2507e-03, -1.1832e-01, -7.3934e-03,\n",
      "         -1.2783e-05, -1.1263e-03, -7.3257e-07,  5.1797e-05, -1.2635e-03],\n",
      "        [ 3.1901e-01, -9.8636e-02,  5.2907e-01, -2.3510e-01, -1.5110e-01,\n",
      "         -9.6773e-01, -3.3772e-01, -7.0292e-02,  1.8937e-01,  1.9134e-03,\n",
      "         -1.0103e-01, -2.6592e-01,  1.0636e-01,  2.2722e-01, -5.3277e-04,\n",
      "         -1.8317e-02, -1.3819e-02, -5.0391e-03, -1.9357e-03, -1.4396e-03,\n",
      "          4.7141e-01, -4.9861e-01,  4.4889e-01, -1.7418e-01, -7.9312e-01,\n",
      "         -1.0331e-01, -1.0634e-01, -2.0320e-01, -5.0103e-01, -2.3760e-02],\n",
      "        [-7.2435e-01,  8.9057e-01, -4.6984e-01, -2.3780e-01,  3.1257e-01,\n",
      "          4.7347e-01, -1.5237e-01,  1.0253e-01,  3.9867e-01, -2.1284e-03,\n",
      "         -9.9436e-02, -1.7369e-01,  8.4815e-02,  3.2841e-01,  9.7114e-04,\n",
      "          2.5081e-02,  1.9090e-02,  6.2541e-03, -1.0350e-02,  1.4499e-03,\n",
      "         -5.3040e-01,  1.4180e+00, -4.8930e-01,  1.4513e-01, -4.0176e-01,\n",
      "         -8.6206e-01,  3.3131e-03,  4.6741e-01, -1.1775e-01,  1.1346e-01],\n",
      "        [-6.1125e-01, -1.8048e-01, -9.2313e-01,  1.5264e-02,  1.1032e-03,\n",
      "         -4.0521e-01, -3.8870e-01, -1.7738e-02,  4.6462e-02, -1.4549e-01,\n",
      "          1.9556e-01, -5.3234e-02,  2.9566e-02,  1.3072e-01,  4.0227e-04,\n",
      "          1.5922e-02, -1.3114e-01,  5.5354e-03,  5.5011e-01,  4.4764e-04,\n",
      "         -9.6554e-01,  2.5342e-01, -7.4311e-01,  2.8965e-01,  6.1247e-01,\n",
      "          1.5747e-01,  4.2229e-02,  3.1241e-01,  8.3163e-02,  7.5836e-02],\n",
      "        [ 8.0891e-01, -1.6712e-01,  7.4085e-01, -3.2121e-02, -1.7510e-01,\n",
      "         -1.0425e-01,  9.1428e-02, -1.9576e-02,  1.7057e-02,  4.8895e-02,\n",
      "          1.5045e-01, -1.3818e-01,  1.2311e-01, -5.3648e-01, -3.0090e-03,\n",
      "         -3.4465e-02, -5.8252e-02, -1.1430e-02,  5.5061e-03, -1.0510e-02,\n",
      "          8.7593e-01, -8.0141e-01,  6.6222e-01, -1.3009e-01,  8.8934e-02,\n",
      "         -3.9116e-01,  2.5331e-01, -2.9781e-01,  1.6405e-01, -3.5032e-01],\n",
      "        [-3.3897e-01, -3.6436e-01, -2.9325e-01,  1.1501e-01, -9.7414e-03,\n",
      "         -3.2272e-03, -2.9477e-01,  2.2484e-02, -7.3228e-03,  1.3894e-01,\n",
      "         -2.8838e-01,  4.8849e-02, -1.6612e-01,  1.0285e-01,  8.5684e-05,\n",
      "         -5.9948e-02,  5.5446e-02,  1.2726e-03, -1.3991e-01,  4.3295e-05,\n",
      "         -4.8460e-01, -1.8463e-01, -2.0625e-01,  2.9264e-01,  6.1275e-01,\n",
      "          3.2343e-01,  1.5379e-01,  1.5997e-02, -2.0274e-01, -5.0209e-03],\n",
      "        [ 2.8651e-01, -1.4689e-01,  1.6142e-01,  2.1170e-01, -1.3986e-02,\n",
      "          3.3286e-01,  2.1344e-01,  1.8430e-01,  2.0964e-01, -4.3329e-01,\n",
      "          4.2074e-01,  2.0726e-01, -7.5732e-02, -1.1694e-02, -1.3758e-03,\n",
      "          3.5043e-02,  5.3804e-02, -5.4595e-03, -2.7695e-01, -3.7312e-02,\n",
      "          4.4647e-01, -6.8675e-01,  1.4004e-01, -2.0908e-01,  1.2263e-01,\n",
      "          3.7829e-01, -1.2008e-01,  3.2649e-01,  6.9528e-02, -9.1971e-02],\n",
      "        [-3.3206e-01,  5.8833e-01, -6.3636e-01, -1.9603e-01,  3.9188e-01,\n",
      "          6.3815e-02,  1.9282e-01, -1.0003e-01,  7.8953e-03,  8.7609e-03,\n",
      "         -1.2170e-01, -1.8796e-04,  9.2148e-02,  1.5854e-01,  3.2570e-03,\n",
      "          3.9827e-02,  1.4064e-01,  1.0117e-02, -4.4571e-03,  2.8956e-02,\n",
      "         -5.2567e-01,  1.2207e+00, -1.9641e-01,  1.9862e-02, -5.3763e-01,\n",
      "          5.7404e-01, -2.4729e-01, -4.2160e-03, -2.9430e-02,  5.7105e-01],\n",
      "        [ 1.8247e-02, -5.6973e-02, -6.7235e-02, -4.0005e-02, -2.0847e-05,\n",
      "         -4.4655e-05, -1.9955e-04, -8.6749e-06,  9.4130e-02,  3.2886e-01,\n",
      "          4.2987e-02, -3.9238e-02, -1.0583e-01,  1.4664e-01, -7.9785e-06,\n",
      "         -5.4084e-06, -5.4350e-06, -5.7125e-06, -3.0522e-06, -7.0665e-06,\n",
      "          2.5040e-03, -6.5315e-02, -8.7185e-02,  3.6038e-02,  1.6913e-01,\n",
      "         -6.0108e-04,  5.2352e-01, -9.8417e-02, -3.8173e-04, -2.4468e-05],\n",
      "        [-5.3886e-39, -4.3019e-39, -6.2563e-39, -6.4146e-39,  9.0411e-40,\n",
      "         -9.6225e-40,  3.0591e-39,  1.6149e-39,  5.5272e-39,  1.5067e-39,\n",
      "          3.7262e-39, -3.0178e-39, -4.6800e-30, -6.7209e-39, -2.4647e-39,\n",
      "         -4.5099e-39, -8.5510e-40, -6.4157e-39, -2.3021e-39, -4.0293e-39,\n",
      "         -6.9435e-39, -3.8859e-39, -6.9372e-39, -6.4264e-39,  4.5867e-39,\n",
      "          1.4318e-40,  2.6033e-39,  2.1603e-39,  3.0805e-39,  2.1245e-39],\n",
      "        [-3.7666e-39, -4.0343e-39, -4.2125e-39, -4.2901e-39,  1.4823e-39,\n",
      "          1.3413e-39,  1.7124e-39,  5.0313e-39, -3.4952e-39, -2.0221e-42,\n",
      "          2.9656e-39, -3.5743e-39, -1.2849e-13, -6.1347e-39, -1.7030e-39,\n",
      "         -5.5394e-39,  2.0095e-39, -2.3202e-39,  6.2942e-39, -7.6989e-40,\n",
      "         -3.7789e-39, -4.3182e-39, -4.0071e-39, -4.4802e-39,  5.2895e-39,\n",
      "         -3.4885e-39, -1.4867e-39,  1.1503e-41,  6.2986e-40, -9.4802e-40]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(pesos0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 828,
     "status": "ok",
     "timestamp": 1595025747100,
     "user": {
      "displayName": "Matheus Pereira de Novaes",
      "photoUrl": "",
      "userId": "01574286574804862444"
     },
     "user_tz": 180
    },
    "id": "iWoRnxSW8GjV",
    "outputId": "9d404d23-3d2b-4aee-dfeb-e76b35ccf6e3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias0 =params[1]\n",
    "bias0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 729,
     "status": "ok",
     "timestamp": 1595025789644,
     "user": {
      "displayName": "Matheus Pereira de Novaes",
      "photoUrl": "",
      "userId": "01574286574804862444"
     },
     "user_tz": 180
    },
    "id": "O6Vv8RBi8ZAr",
    "outputId": "6c3ce0c2-16ee-4ca0-b702-1bc552e7641a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 16])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pesos1 = params[2]\n",
    "pesos1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1kHDQG4M8lVl"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cK2ph77-kNQp"
   },
   "source": [
    "## Etapa 7: Fase de Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 163
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 761,
     "status": "ok",
     "timestamp": 1595025849237,
     "user": {
      "displayName": "Matheus Pereira de Novaes",
      "photoUrl": "",
      "userId": "01574286574804862444"
     },
     "user_tz": 180
    },
    "id": "mZRfgEqZ8sQS",
    "outputId": "5cde9058-1a5b-4655-a23c-5a55490f2e41"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=30, out_features=16, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=16, out_features=16, bias=True)\n",
       "  (3): ReLU()\n",
       "  (4): Linear(in_features=16, out_features=1, bias=True)\n",
       "  (5): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1024,
     "status": "ok",
     "timestamp": 1595025952833,
     "user": {
      "displayName": "Matheus Pereira de Novaes",
      "photoUrl": "",
      "userId": "01574286574804862444"
     },
     "user_tz": 180
    },
    "id": "RSdGhe-R82oH"
   },
   "outputs": [],
   "source": [
    "X_teste = torch.tensor(np.array(X_teste), dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1260,
     "status": "ok",
     "timestamp": 1595025969685,
     "user": {
      "displayName": "Matheus Pereira de Novaes",
      "photoUrl": "",
      "userId": "01574286574804862444"
     },
     "user_tz": 180
    },
    "id": "_P6HUu929KWE",
    "outputId": "81d66b1b-4747-4a99-93e1-3a54b1ea6c0f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2277,
     "status": "ok",
     "timestamp": 1595026008399,
     "user": {
      "displayName": "Matheus Pereira de Novaes",
      "photoUrl": "",
      "userId": "01574286574804862444"
     },
     "user_tz": 180
    },
    "id": "vo0efzBt9OxN"
   },
   "outputs": [],
   "source": [
    "saidas_teste = model.forward(X_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 990,
     "status": "ok",
     "timestamp": 1595026022405,
     "user": {
      "displayName": "Matheus Pereira de Novaes",
      "photoUrl": "",
      "userId": "01574286574804862444"
     },
     "user_tz": 180
    },
    "id": "h8XmAOmg9Xwl",
    "outputId": "a63cfa38-1b0a-446f-fd4b-ccd185f0b794"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9.9899e-01],\n",
       "        [1.0000e+00],\n",
       "        [9.5384e-01],\n",
       "        [1.0000e+00],\n",
       "        [4.8911e-08],\n",
       "        [1.0000e+00],\n",
       "        [9.9995e-01],\n",
       "        [1.0000e+00],\n",
       "        [9.9834e-01],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [9.9991e-01],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [4.6254e-01],\n",
       "        [7.5863e-02],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [9.4419e-01],\n",
       "        [1.0000e+00],\n",
       "        [1.4321e-19],\n",
       "        [7.8065e-06],\n",
       "        [9.7757e-01],\n",
       "        [1.0000e+00],\n",
       "        [9.9987e-01],\n",
       "        [3.0512e-03],\n",
       "        [1.3336e-01],\n",
       "        [1.0000e+00],\n",
       "        [2.9454e-10],\n",
       "        [1.0000e+00],\n",
       "        [9.6566e-01],\n",
       "        [1.0000e+00],\n",
       "        [9.9999e-01],\n",
       "        [9.9999e-01],\n",
       "        [8.3148e-01],\n",
       "        [1.0000e+00],\n",
       "        [9.9999e-01],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [4.1795e-05],\n",
       "        [3.3552e-02],\n",
       "        [9.5149e-01],\n",
       "        [4.6131e-01],\n",
       "        [8.4359e-01],\n",
       "        [4.0030e-07],\n",
       "        [9.9976e-01],\n",
       "        [6.5653e-07],\n",
       "        [0.0000e+00],\n",
       "        [9.8897e-01],\n",
       "        [2.1977e-01],\n",
       "        [1.0000e+00],\n",
       "        [1.6712e-34],\n",
       "        [9.9969e-01],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [8.4463e-01],\n",
       "        [1.0000e+00],\n",
       "        [6.3642e-08],\n",
       "        [1.5292e-15],\n",
       "        [1.0000e+00],\n",
       "        [4.6011e-08],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [9.7341e-01],\n",
       "        [1.9954e-15],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [9.5344e-01],\n",
       "        [1.0000e+00],\n",
       "        [9.9999e-01],\n",
       "        [5.9733e-01],\n",
       "        [9.6607e-01],\n",
       "        [3.2989e-04],\n",
       "        [9.9975e-01],\n",
       "        [9.9833e-01],\n",
       "        [1.9252e-20],\n",
       "        [2.7610e-16],\n",
       "        [3.6033e-04],\n",
       "        [1.0000e+00],\n",
       "        [1.5768e-06],\n",
       "        [1.1525e-11],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [1.8199e-01],\n",
       "        [1.0000e+00],\n",
       "        [1.8595e-03],\n",
       "        [9.9027e-01],\n",
       "        [2.3911e-03],\n",
       "        [9.9998e-01],\n",
       "        [1.0000e+00],\n",
       "        [3.9380e-04],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [9.9999e-01],\n",
       "        [1.0000e+00],\n",
       "        [2.9945e-03],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [9.2945e-01],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [9.9901e-01],\n",
       "        [2.0288e-07],\n",
       "        [7.4631e-01],\n",
       "        [1.9964e-05],\n",
       "        [1.0000e+00],\n",
       "        [1.1264e-08],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [9.9999e-01],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [5.2401e-14],\n",
       "        [1.0000e+00],\n",
       "        [6.7310e-01],\n",
       "        [3.0786e-02],\n",
       "        [1.0000e+00],\n",
       "        [1.6875e-01],\n",
       "        [2.1459e-18],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [7.0038e-03],\n",
       "        [1.7808e-23],\n",
       "        [9.9986e-01],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [3.8516e-01],\n",
       "        [8.4614e-09],\n",
       "        [3.6421e-07],\n",
       "        [1.0000e+00],\n",
       "        [2.7546e-23]], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saidas_teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1628,
     "status": "ok",
     "timestamp": 1595026146755,
     "user": {
      "displayName": "Matheus Pereira de Novaes",
      "photoUrl": "",
      "userId": "01574286574804862444"
     },
     "user_tz": 180
    },
    "id": "IvoG3llF9uJs"
   },
   "outputs": [],
   "source": [
    "saidas_teste = np.array(saidas_teste > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 847,
     "status": "ok",
     "timestamp": 1595026256374,
     "user": {
      "displayName": "Matheus Pereira de Novaes",
      "photoUrl": "",
      "userId": "01574286574804862444"
     },
     "user_tz": 180
    },
    "id": "zAjEqiRw-CjR",
    "outputId": "838f6e68-2130-4f6a-e87c-28048783ef39"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8881118881118881"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxa_acerto = accuracy_score(y_teste,saidas_teste)\n",
    "taxa_acerto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1558,
     "status": "ok",
     "timestamp": 1595026352186,
     "user": {
      "displayName": "Matheus Pereira de Novaes",
      "photoUrl": "",
      "userId": "01574286574804862444"
     },
     "user_tz": 180
    },
    "id": "hhd36nYA-Vwf"
   },
   "outputs": [],
   "source": [
    "matriz = confusion_matrix(y_teste,saidas_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1042,
     "status": "ok",
     "timestamp": 1595026393442,
     "user": {
      "displayName": "Matheus Pereira de Novaes",
      "photoUrl": "",
      "userId": "01574286574804862444"
     },
     "user_tz": 180
    },
    "id": "N4LNP4c3-raA",
    "outputId": "354832b9-33a5-4078-8c4e-23c1c2063832"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(matriz, annot=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_VTs2Blyoo6A"
   },
   "source": [
    "# Parte 2: Validação cruzada e dropout com o PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tstEbZxao06J"
   },
   "source": [
    "## Etapa 1: Importacação das bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch import NeuralNetBinaryClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XLHiT93jqrS4"
   },
   "source": [
    "## Etapa 2: Base de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569,)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasetInput = np.array(datasetInput, dtype='float32')\n",
    "datasetInput.shape\n",
    "\n",
    "datasetOuput = np.array(datasetOuput, dtype='float32').squeeze(1)\n",
    "datasetOuput.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a9v8UVNMqN2s"
   },
   "source": [
    "## Etapa 3: Definir uma classe para estrutura da rede"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tem que herda a classe nn.Module\n",
    "class model_nn_torch(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 30 ->16 ->16 ->1\n",
    "        self.dense0 = nn.Linear(30, 16)\n",
    "        torch.nn.init.uniform_(self.dense0.weight) # Pesos iniciando com distribuicao uniforme\n",
    "        self.activation0 = nn.ReLU()\n",
    "        self.dense1 = nn.Linear(16, 16)\n",
    "        torch.nn.init.uniform_(self.dense1.weight)\n",
    "        self.activation1 = nn.ReLU()\n",
    "        self.dense2 = nn.Linear(16, 1)\n",
    "        torch.nn.init.uniform_(self.dense2.weight)\n",
    "        self.output = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, X):\n",
    "        X = self.dense0(X)\n",
    "        X = self.activation0(X)\n",
    "        X = self.dense1(X)\n",
    "        X = self.activation1(X)\n",
    "        X = self.dense2(X)\n",
    "        X = self.output(X)\n",
    "        return X\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hsZQ7x36xd0I"
   },
   "source": [
    "## Etapa 4: Skorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_sklearn = NeuralNetBinaryClassifier(module=model_nn_torch, \n",
    "                                      criterion=torch.nn.BCELoss,\n",
    "                                      optimizer=torch.optim.Adam,\n",
    "                                      lr=0.0001,\n",
    "                                      optimizer__weight_decay = 0.001,\n",
    "                                      max_epochs=100,\n",
    "                                      batch_size=10,\n",
    "                                      train_split=False # Para ser executado pelo sklearn\n",
    "                                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1q-pkhO9zmFk"
   },
   "source": [
    "## Etap 5: Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       37.1094  0.0782\n",
      "      2       37.1094  0.0539\n",
      "      3       37.1094  0.0499\n",
      "      4       37.1094  0.0509\n",
      "      5       37.1094  0.0539\n",
      "      6       37.1094  0.0559\n",
      "      7       37.1094  0.0598\n",
      "      8       37.1094  0.0608\n",
      "      9       37.1094  0.0598\n",
      "     10       37.1094  0.0539\n",
      "     11       37.1094  0.0529\n",
      "     12       37.1094  0.0499\n",
      "     13       37.1094  0.0509\n",
      "     14       37.1094  0.0499\n",
      "     15       37.1094  0.0509\n",
      "     16       37.1094  0.0503\n",
      "     17       37.1094  0.0509\n",
      "     18       37.1094  0.0519\n",
      "     19       37.1094  0.0509\n",
      "     20       37.1094  0.0509\n",
      "     21       37.1094  0.0519\n",
      "     22       37.1094  0.0539\n",
      "     23       37.1094  0.0549\n",
      "     24       37.1094  0.0519\n",
      "     25       37.1094  0.0539\n",
      "     26       37.1094  0.0519\n",
      "     27       37.1094  0.0509\n",
      "     28       37.1094  0.0499\n",
      "     29       37.1094  0.0519\n",
      "     30       37.1094  0.0529\n",
      "     31       37.1094  0.0578\n",
      "     32       37.1094  0.0568\n",
      "     33       37.1094  0.0549\n",
      "     34       37.1094  0.0549\n",
      "     35       37.1094  0.0529\n",
      "     36       37.1094  0.0519\n",
      "     37       37.1094  0.0578\n",
      "     38       37.1094  0.0559\n",
      "     39       37.1094  0.0549\n",
      "     40       37.1094  0.0568\n",
      "     41       37.1094  0.0539\n",
      "     42       37.1094  0.0549\n",
      "     43       37.1094  0.0559\n",
      "     44       37.1094  0.0549\n",
      "     45       37.1094  0.0529\n",
      "     46       37.1094  0.0563\n",
      "     47       37.1094  0.0633\n",
      "     48       37.1094  0.0578\n",
      "     49       37.1094  0.0539\n",
      "     50       37.1094  0.0539\n",
      "     51       37.1094  0.0499\n",
      "     52       37.1094  0.0509\n",
      "     53       37.1094  0.0529\n",
      "     54       37.1094  0.0529\n",
      "     55       37.1094  0.0529\n",
      "     56       37.1094  0.0529\n",
      "     57       37.1094  0.0519\n",
      "     58       37.1094  0.0539\n",
      "     59       37.1094  0.0529\n",
      "     60       37.1094  0.0548\n",
      "     61       37.1094  0.0534\n",
      "     62       37.1094  0.0538\n",
      "     63       37.1094  0.0529\n",
      "     64       37.1094  0.0519\n",
      "     65       37.1094  0.0538\n",
      "     66       37.1094  0.0529\n",
      "     67       37.1094  0.0569\n",
      "     68       37.1094  0.0529\n",
      "     69       37.1094  0.0548\n",
      "     70       37.1094  0.0598\n",
      "     71       37.1094  0.0568\n",
      "     72       37.1094  0.0554\n",
      "     73       37.1094  0.0519\n",
      "     74       37.1094  0.0519\n",
      "     75       37.1094  0.0509\n",
      "     76       37.1094  0.0519\n",
      "     77       37.1094  0.0529\n",
      "     78       37.1094  0.0519\n",
      "     79       37.1094  0.0519\n",
      "     80       37.1094  0.0539\n",
      "     81       37.1094  0.0529\n",
      "     82       37.1094  0.0519\n",
      "     83       37.1094  0.0568\n",
      "     84       37.1094  0.0588\n",
      "     85       37.1094  0.0568\n",
      "     86       37.1094  0.0561\n",
      "     87       37.1094  0.0529\n",
      "     88       37.1094  0.0519\n",
      "     89       37.1094  0.0556\n",
      "     90       37.1094  0.0549\n",
      "     91       37.1094  0.0568\n",
      "     92       37.1094  0.0568\n",
      "     93       37.1094  0.0597\n",
      "     94       37.1094  0.0613\n",
      "     95       37.1094  0.0549\n",
      "     96       37.1094  0.0529\n",
      "     97       37.1094  0.0519\n",
      "     98       37.1094  0.0568\n",
      "     99       37.1094  0.0559\n",
      "    100       37.1094  0.0569\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       37.1094  0.0539\n",
      "      2       37.1094  0.0559\n",
      "      3       37.1094  0.0509\n",
      "      4       37.1094  0.0489\n",
      "      5       37.1094  0.0509\n",
      "      6       37.1094  0.0508\n",
      "      7       37.1094  0.0549\n",
      "      8       37.1094  0.0569\n",
      "      9       37.1094  0.0568\n",
      "     10       37.1094  0.0549\n",
      "     11       37.1094  0.0559\n",
      "     12       37.1094  0.0549\n",
      "     13       37.1094  0.0519\n",
      "     14       37.1094  0.0519\n",
      "     15       37.1094  0.0519\n",
      "     16       37.1094  0.0509\n",
      "     17       37.1094  0.0489\n",
      "     18       37.1094  0.0499\n",
      "     19       37.1094  0.0509\n",
      "     20       37.1094  0.0518\n",
      "     21       37.1094  0.0499\n",
      "     22       37.1094  0.0509\n",
      "     23       37.1094  0.0569\n",
      "     24       37.1094  0.0539\n",
      "     25       37.1094  0.0539\n",
      "     26       37.1094  0.0539\n",
      "     27       37.1094  0.0519\n",
      "     28       37.1094  0.0489\n",
      "     29       37.1094  0.0519\n",
      "     30       37.1094  0.0519\n",
      "     31       37.1094  0.0539\n",
      "     32       37.1094  0.0529\n",
      "     33       37.1094  0.0519\n",
      "     34       37.1094  0.0509\n",
      "     35       37.1094  0.0519\n",
      "     36       37.1094  0.0529\n",
      "     37       37.1094  0.0509\n",
      "     38       37.1094  0.0499\n",
      "     39       37.1094  0.0499\n",
      "     40       37.1094  0.0509\n",
      "     41       37.1094  0.0497\n",
      "     42       37.1094  0.0518\n",
      "     43       37.1094  0.0509\n",
      "     44       37.1094  0.0521\n",
      "     45       37.1094  0.0529\n",
      "     46       37.1094  0.0519\n",
      "     47       37.1094  0.0523\n",
      "     48       37.1094  0.0519\n",
      "     49       37.1094  0.0510\n",
      "     50       37.1094  0.0514\n",
      "     51       37.1094  0.0519\n",
      "     52       37.1094  0.0529\n",
      "     53       37.1094  0.0519\n",
      "     54       37.1094  0.0519\n",
      "     55       37.1094  0.0539\n",
      "     56       37.1094  0.0549\n",
      "     57       37.1094  0.0549\n",
      "     58       37.1094  0.0568\n",
      "     59       37.1094  0.0559\n",
      "     60       37.1094  0.0568\n",
      "     61       37.1094  0.0598\n",
      "     62       37.1094  0.0568\n",
      "     63       37.1094  0.0568\n",
      "     64       37.1094  0.0573\n",
      "     65       37.1094  0.0581\n",
      "     66       37.1094  0.0529\n",
      "     67       37.1094  0.0539\n",
      "     68       37.1094  0.0534\n",
      "     69       37.1094  0.0509\n",
      "     70       37.1094  0.0519\n",
      "     71       37.1094  0.0499\n",
      "     72       37.1094  0.0509\n",
      "     73       37.1094  0.0553\n",
      "     74       37.1094  0.0529\n",
      "     75       37.1094  0.0529\n",
      "     76       37.1094  0.0568\n",
      "     77       37.1094  0.0568\n",
      "     78       37.1094  0.0558\n",
      "     79       37.1094  0.0529\n",
      "     80       37.1094  0.0499\n",
      "     81       37.1094  0.0519\n",
      "     82       37.1094  0.0529\n",
      "     83       37.1094  0.0538\n",
      "     84       37.1094  0.0578\n",
      "     85       37.1094  0.0549\n",
      "     86       37.1094  0.0559\n",
      "     87       37.1094  0.0558\n",
      "     88       37.1094  0.0557\n",
      "     89       37.1094  0.0549\n",
      "     90       37.1094  0.0528\n",
      "     91       37.1094  0.0539\n",
      "     92       37.1094  0.0558\n",
      "     93       37.1094  0.0608\n",
      "     94       37.1094  0.0559\n",
      "     95       37.1094  0.0578\n",
      "     96       37.1094  0.0578\n",
      "     97       37.1094  0.0588\n",
      "     98       37.1094  0.0578\n",
      "     99       37.1094  0.0549\n",
      "    100       37.1094  0.0559\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       37.3047  0.0519\n",
      "      2       37.3047  0.0539\n",
      "      3       37.3047  0.0509\n",
      "      4       37.3047  0.0529\n",
      "      5       37.3047  0.0509\n",
      "      6       37.3047  0.0519\n",
      "      7       37.3047  0.0519\n",
      "      8       37.3047  0.0499\n",
      "      9       37.3047  0.0509\n",
      "     10       37.3047  0.0509\n",
      "     11       37.3047  0.0519\n",
      "     12       37.3047  0.0578\n",
      "     13       37.3047  0.0531\n",
      "     14       37.3047  0.0531\n",
      "     15       37.3047  0.0628\n",
      "     16       37.3047  0.0484\n",
      "     17       37.3047  0.0568\n",
      "     18       37.3047  0.1287\n",
      "     19       37.3047  0.0608\n",
      "     20       37.3047  0.0550\n",
      "     21       37.3047  0.0643\n",
      "     22       37.3047  0.0549\n",
      "     23       37.3047  0.0499\n",
      "     24       37.3047  0.0509\n",
      "     25       37.3047  0.0509\n",
      "     26       37.3047  0.0519\n",
      "     27       37.3047  0.0509\n",
      "     28       37.3047  0.0529\n",
      "     29       37.3047  0.0538\n",
      "     30       37.3047  0.0519\n",
      "     31       37.3047  0.0538\n",
      "     32       37.3047  0.0519\n",
      "     33       37.3047  0.0608\n",
      "     34       37.3047  0.0519\n",
      "     35       37.3047  0.0568\n",
      "     36       37.3047  0.0549\n",
      "     37       37.3047  0.0578\n",
      "     38       37.3047  0.0579\n",
      "     39       37.3047  0.0549\n",
      "     40       37.3047  0.0509\n",
      "     41       37.3047  0.0529\n",
      "     42       37.3047  0.0529\n",
      "     43       37.3047  0.0529\n",
      "     44       37.3047  0.0519\n",
      "     45       37.3047  0.0518\n",
      "     46       37.3047  0.0530\n",
      "     47       37.3047  0.0509\n",
      "     48       37.3047  0.0513\n",
      "     49       37.3047  0.0512\n",
      "     50       37.3047  0.0518\n",
      "     51       37.3047  0.0519\n",
      "     52       37.3047  0.0519\n",
      "     53       37.3047  0.0529\n",
      "     54       37.3047  0.0519\n",
      "     55       37.3047  0.0609\n",
      "     56       37.3047  0.0648\n",
      "     57       37.3047  0.0718\n",
      "     58       37.3047  0.0718\n",
      "     59       37.3047  0.0698\n",
      "     60       37.3047  0.0638\n",
      "     61       37.3047  0.0568\n",
      "     62       37.3047  0.0529\n",
      "     63       37.3047  0.0549\n",
      "     64       37.3047  0.0559\n",
      "     65       37.3047  0.0569\n",
      "     66       37.3047  0.0568\n",
      "     67       37.3047  0.0549\n",
      "     68       37.3047  0.0566\n",
      "     69       37.3047  0.0575\n",
      "     70       37.3047  0.0558\n",
      "     71       37.3047  0.0559\n",
      "     72       37.3047  0.0539\n",
      "     73       37.3047  0.0558\n",
      "     74       37.3047  0.0528\n",
      "     75       37.3047  0.0568\n",
      "     76       37.3047  0.0529\n",
      "     77       37.3047  0.0529\n",
      "     78       37.3047  0.0538\n",
      "     79       37.3047  0.0539\n",
      "     80       37.3047  0.0528\n",
      "     81       37.3047  0.0519\n",
      "     82       37.3047  0.0539\n",
      "     83       37.3047  0.0529\n",
      "     84       37.3047  0.0539\n",
      "     85       37.3047  0.0539\n",
      "     86       37.3047  0.0549\n",
      "     87       37.3047  0.0539\n",
      "     88       37.3047  0.0539\n",
      "     89       37.3047  0.0559\n",
      "     90       37.3047  0.0549\n",
      "     91       37.3047  0.0588\n",
      "     92       37.3047  0.0628\n",
      "     93       37.3047  0.0578\n",
      "     94       37.3047  0.0568\n",
      "     95       37.3047  0.0568\n",
      "     96       37.3047  0.0549\n",
      "     97       37.3047  0.0549\n",
      "     98       37.3047  0.0579\n",
      "     99       37.3047  0.0549\n",
      "    100       37.3047  0.0558\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       37.3047  0.0509\n",
      "      2       37.3047  0.0529\n",
      "      3       37.3047  0.0528\n",
      "      4       37.3047  0.0509\n",
      "      5       37.3047  0.0539\n",
      "      6       37.3047  0.0529\n",
      "      7       37.3047  0.0528\n",
      "      8       37.3047  0.0529\n",
      "      9       37.3047  0.0528\n",
      "     10       37.3047  0.0509\n",
      "     11       37.3047  0.0529\n",
      "     12       37.3047  0.0519\n",
      "     13       37.3047  0.0528\n",
      "     14       37.3047  0.0539\n",
      "     15       37.3047  0.0509\n",
      "     16       37.3047  0.0529\n",
      "     17       37.3047  0.0539\n",
      "     18       37.3047  0.0529\n",
      "     19       37.3047  0.0529\n",
      "     20       37.3047  0.0519\n",
      "     21       37.3047  0.0529\n",
      "     22       37.3047  0.0509\n",
      "     23       37.3047  0.0529\n",
      "     24       37.3047  0.0529\n",
      "     25       37.3047  0.0528\n",
      "     26       37.3047  0.0509\n",
      "     27       37.3047  0.0529\n",
      "     28       37.3047  0.0519\n",
      "     29       37.3047  0.0519\n",
      "     30       37.3047  0.0529\n",
      "     31       37.3047  0.0515\n",
      "     32       37.3047  0.0529\n",
      "     33       37.3047  0.0519\n",
      "     34       37.3047  0.0525\n",
      "     35       37.3047  0.0539\n",
      "     36       37.3047  0.0538\n",
      "     37       37.3047  0.0519\n",
      "     38       37.3047  0.0539\n",
      "     39       37.3047  0.0531\n",
      "     40       37.3047  0.0549\n",
      "     41       37.3047  0.0529\n",
      "     42       37.3047  0.0539\n",
      "     43       37.3047  0.0518\n",
      "     44       37.3047  0.0539\n",
      "     45       37.3047  0.0569\n",
      "     46       37.3047  0.0578\n",
      "     47       37.3047  0.0559\n",
      "     48       37.3047  0.0549\n",
      "     49       37.3047  0.0559\n",
      "     50       37.3047  0.0569\n",
      "     51       37.3047  0.0549\n",
      "     52       37.3047  0.0539\n",
      "     53       37.3047  0.0549\n",
      "     54       37.3047  0.0559\n",
      "     55       37.3047  0.0568\n",
      "     56       37.3047  0.0549\n",
      "     57       37.3047  0.0538\n",
      "     58       37.3047  0.0538\n",
      "     59       37.3047  0.0558\n",
      "     60       37.3047  0.0659\n",
      "     61       37.3047  0.0608\n",
      "     62       37.3047  0.0539\n",
      "     63       37.3047  0.0539\n",
      "     64       37.3047  0.0588\n",
      "     65       37.3047  0.0588\n",
      "     66       37.3047  0.0688\n",
      "     67       37.3047  0.0539\n",
      "     68       37.3047  0.0519\n",
      "     69       37.3047  0.0558\n",
      "     70       37.3047  0.0548\n",
      "     71       37.3047  0.0558\n",
      "     72       37.3047  0.0558\n",
      "     73       37.3047  0.0548\n",
      "     74       37.3047  0.0549\n",
      "     75       37.3047  0.0539\n",
      "     76       37.3047  0.0552\n",
      "     77       37.3047  0.0558\n",
      "     78       37.3047  0.0549\n",
      "     79       37.3047  0.0538\n",
      "     80       37.3047  0.0558\n",
      "     81       37.3047  0.0549\n",
      "     82       37.3047  0.0555\n",
      "     83       37.3047  0.0543\n",
      "     84       37.3047  0.0574\n",
      "     85       37.3047  0.0549\n",
      "     86       37.3047  0.0549\n",
      "     87       37.3047  0.0539\n",
      "     88       37.3047  0.0568\n",
      "     89       37.3047  0.0549\n",
      "     90       37.3047  0.0559\n",
      "     91       37.3047  0.0559\n",
      "     92       37.3047  0.0559\n",
      "     93       37.3047  0.0559\n",
      "     94       37.3047  0.0559\n",
      "     95       37.3047  0.0549\n",
      "     96       37.3047  0.0539\n",
      "     97       37.3047  0.0559\n",
      "     98       37.3047  0.0564\n",
      "     99       37.3047  0.0550\n",
      "    100       37.3047  0.0559\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       37.3047  0.0529\n",
      "      2       37.3047  0.0549\n",
      "      3       37.3047  0.0529\n",
      "      4       37.3047  0.0549\n",
      "      5       37.3047  0.0549\n",
      "      6       37.3047  0.0538\n",
      "      7       37.3047  0.0539\n",
      "      8       37.3047  0.0539\n",
      "      9       37.3047  0.0529\n",
      "     10       37.3047  0.0519\n",
      "     11       37.3047  0.0549\n",
      "     12       37.3047  0.0559\n",
      "     13       37.3047  0.0529\n",
      "     14       37.3047  0.0528\n",
      "     15       37.3047  0.0528\n",
      "     16       37.3047  0.0529\n",
      "     17       37.3047  0.0539\n",
      "     18       37.3047  0.0519\n",
      "     19       37.3047  0.0539\n",
      "     20       37.3047  0.0529\n",
      "     21       37.3047  0.0529\n",
      "     22       37.3047  0.0529\n",
      "     23       37.3047  0.0538\n",
      "     24       37.3047  0.0549\n",
      "     25       37.3047  0.0535\n",
      "     26       37.3047  0.0529\n",
      "     27       37.3047  0.0538\n",
      "     28       37.3047  0.0549\n",
      "     29       37.3047  0.0519\n",
      "     30       37.3047  0.0525\n",
      "     31       37.3047  0.0548\n",
      "     32       37.3047  0.0532\n",
      "     33       37.3047  0.0539\n",
      "     34       37.3047  0.0518\n",
      "     35       37.3047  0.0529\n",
      "     36       37.3047  0.0529\n",
      "     37       37.3047  0.0539\n",
      "     38       37.3047  0.0529\n",
      "     39       37.3047  0.0529\n",
      "     40       37.3047  0.0539\n",
      "     41       37.3047  0.0539\n",
      "     42       37.3047  0.0539\n",
      "     43       37.3047  0.0538\n",
      "     44       37.3047  0.0539\n",
      "     45       37.3047  0.0529\n",
      "     46       37.3047  0.0533\n",
      "     47       37.3047  0.0539\n",
      "     48       37.3047  0.0532\n",
      "     49       37.3047  0.0539\n",
      "     50       37.3047  0.0539\n",
      "     51       37.3047  0.0525\n",
      "     52       37.3047  0.0538\n",
      "     53       37.3047  0.0529\n",
      "     54       37.3047  0.0529\n",
      "     55       37.3047  0.0549\n",
      "     56       37.3047  0.0554\n",
      "     57       37.3047  0.0525\n",
      "     58       37.3047  0.0519\n",
      "     59       37.3047  0.0539\n",
      "     60       37.3047  0.0548\n",
      "     61       37.3047  0.0529\n",
      "     62       37.3047  0.0549\n",
      "     63       37.3047  0.0529\n",
      "     64       37.3047  0.0538\n",
      "     65       37.3047  0.0539\n",
      "     66       37.3047  0.0529\n",
      "     67       37.3047  0.0539\n",
      "     68       37.3047  0.0539\n",
      "     69       37.3047  0.0539\n",
      "     70       37.3047  0.0559\n",
      "     71       37.3047  0.0539\n",
      "     72       37.3047  0.0544\n",
      "     73       37.3047  0.0529\n",
      "     74       37.3047  0.0549\n",
      "     75       37.3047  0.0549\n",
      "     76       37.3047  0.0539\n",
      "     77       37.3047  0.0539\n",
      "     78       37.3047  0.0545\n",
      "     79       37.3047  0.0539\n",
      "     80       37.3047  0.0539\n",
      "     81       37.3047  0.0529\n",
      "     82       37.3047  0.0549\n",
      "     83       37.3047  0.0549\n",
      "     84       37.3047  0.0549\n",
      "     85       37.3047  0.0529\n",
      "     86       37.3047  0.0559\n",
      "     87       37.3047  0.0529\n",
      "     88       37.3047  0.0549\n",
      "     89       37.3047  0.0558\n",
      "     90       37.3047  0.0539\n",
      "     91       37.3047  0.0539\n",
      "     92       37.3047  0.0549\n",
      "     93       37.3047  0.0538\n",
      "     94       37.3047  0.0539\n",
      "     95       37.3047  0.0535\n",
      "     96       37.3047  0.0558\n",
      "     97       37.3047  0.0539\n",
      "     98       37.3047  0.0548\n",
      "     99       37.3047  0.0565\n",
      "    100       37.3047  0.0551\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       37.3047  0.0509\n",
      "      2       37.3047  0.0549\n",
      "      3       37.3047  0.0529\n",
      "      4       37.3047  0.0529\n",
      "      5       37.3047  0.0538\n",
      "      6       37.3047  0.0529\n",
      "      7       37.3047  0.0529\n",
      "      8       37.3047  0.0559\n",
      "      9       37.3047  0.0539\n",
      "     10       37.3047  0.0529\n",
      "     11       37.3047  0.0539\n",
      "     12       37.3047  0.0538\n",
      "     13       37.3047  0.0529\n",
      "     14       37.3047  0.0538\n",
      "     15       37.3047  0.0539\n",
      "     16       37.3047  0.0554\n",
      "     17       37.3047  0.0529\n",
      "     18       37.3047  0.0529\n",
      "     19       37.3047  0.0522\n",
      "     20       37.3047  0.0519\n",
      "     21       37.3047  0.0533\n",
      "     22       37.3047  0.0539\n",
      "     23       37.3047  0.0539\n",
      "     24       37.3047  0.0529\n",
      "     25       37.3047  0.0529\n",
      "     26       37.3047  0.0529\n",
      "     27       37.3047  0.0519\n",
      "     28       37.3047  0.0530\n",
      "     29       37.3047  0.0531\n",
      "     30       37.3047  0.0529\n",
      "     31       37.3047  0.0540\n",
      "     32       37.3047  0.0529\n",
      "     33       37.3047  0.0549\n",
      "     34       37.3047  0.0529\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     35       37.3047  0.0529\n",
      "     36       37.3047  0.0539\n",
      "     37       37.3047  0.0519\n",
      "     38       37.3047  0.0529\n",
      "     39       37.3047  0.0539\n",
      "     40       37.3047  0.0529\n",
      "     41       37.3047  0.0529\n",
      "     42       37.3047  0.0519\n",
      "     43       37.3047  0.0519\n",
      "     44       37.3047  0.0529\n",
      "     45       37.3047  0.0539\n",
      "     46       37.3047  0.0548\n",
      "     47       37.3047  0.0532\n",
      "     48       37.3047  0.0539\n",
      "     49       37.3047  0.0568\n",
      "     50       37.3047  0.0539\n",
      "     51       37.3047  0.0539\n",
      "     52       37.3047  0.0529\n",
      "     53       37.3047  0.0519\n",
      "     54       37.3047  0.0549\n",
      "     55       37.3047  0.1182\n",
      "     56       37.3047  0.0953\n",
      "     57       37.3047  0.0584\n",
      "     58       37.3047  0.0947\n",
      "     59       37.3047  0.0628\n",
      "     60       37.3047  0.0628\n",
      "     61       37.3047  0.0568\n",
      "     62       37.3047  0.0608\n",
      "     63       37.3047  0.0568\n",
      "     64       37.3047  0.0559\n",
      "     65       37.3047  0.0509\n",
      "     66       37.3047  0.0678\n",
      "     67       37.3047  0.0618\n",
      "     68       37.3047  0.0539\n",
      "     69       37.3047  0.0519\n",
      "     70       37.3047  0.0529\n",
      "     71       37.3047  0.0598\n",
      "     72       37.3047  0.0559\n",
      "     73       37.3047  0.0559\n",
      "     74       37.3047  0.0618\n",
      "     75       37.3047  0.0519\n",
      "     76       37.3047  0.0509\n",
      "     77       37.3047  0.0519\n",
      "     78       37.3047  0.0529\n",
      "     79       37.3047  0.0529\n",
      "     80       37.3047  0.0538\n",
      "     81       37.3047  0.0518\n",
      "     82       37.3047  0.0529\n",
      "     83       37.3047  0.0538\n",
      "     84       37.3047  0.0529\n",
      "     85       37.3047  0.0548\n",
      "     86       37.3047  0.0539\n",
      "     87       37.3047  0.0529\n",
      "     88       37.3047  0.0539\n",
      "     89       37.3047  0.0546\n",
      "     90       37.3047  0.0539\n",
      "     91       37.3047  0.0538\n",
      "     92       37.3047  0.0529\n",
      "     93       37.3047  0.0539\n",
      "     94       37.3047  0.0558\n",
      "     95       37.3047  0.0549\n",
      "     96       37.3047  0.0539\n",
      "     97       37.3047  0.0538\n",
      "     98       37.3047  0.0549\n",
      "     99       37.3047  0.0529\n",
      "    100       37.3047  0.0529\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       37.3047  0.0509\n",
      "      2       37.3047  0.0519\n",
      "      3       37.3047  0.0529\n",
      "      4       37.3047  0.0519\n",
      "      5       37.3047  0.0509\n",
      "      6       37.3047  0.0499\n",
      "      7       37.3047  0.0509\n",
      "      8       37.3047  0.0518\n",
      "      9       37.3047  0.0519\n",
      "     10       37.3047  0.0519\n",
      "     11       37.3047  0.0499\n",
      "     12       37.3047  0.0509\n",
      "     13       37.3047  0.0509\n",
      "     14       37.3047  0.0529\n",
      "     15       37.3047  0.0499\n",
      "     16       37.3047  0.0521\n",
      "     17       37.3047  0.0519\n",
      "     18       37.3047  0.0509\n",
      "     19       37.3047  0.0519\n",
      "     20       37.3047  0.0518\n",
      "     21       37.3047  0.0519\n",
      "     22       37.3047  0.0509\n",
      "     23       37.3047  0.0509\n",
      "     24       37.3047  0.0509\n",
      "     25       37.3047  0.0519\n",
      "     26       37.3047  0.0519\n",
      "     27       37.3047  0.0509\n",
      "     28       37.3047  0.0509\n",
      "     29       37.3047  0.0509\n",
      "     30       37.3047  0.0519\n",
      "     31       37.3047  0.0509\n",
      "     32       37.3047  0.0509\n",
      "     33       37.3047  0.0509\n",
      "     34       37.3047  0.0534\n",
      "     35       37.3047  0.0541\n",
      "     36       37.3047  0.0549\n",
      "     37       37.3047  0.0581\n",
      "     38       37.3047  0.0578\n",
      "     39       37.3047  0.0499\n",
      "     40       37.3047  0.0519\n",
      "     41       37.3047  0.0528\n",
      "     42       37.3047  0.0519\n",
      "     43       37.3047  0.0511\n",
      "     44       37.3047  0.0519\n",
      "     45       37.3047  0.0534\n",
      "     46       37.3047  0.0529\n",
      "     47       37.3047  0.0509\n",
      "     48       37.3047  0.0518\n",
      "     49       37.3047  0.0519\n",
      "     50       37.3047  0.0529\n",
      "     51       37.3047  0.0509\n",
      "     52       37.3047  0.0519\n",
      "     53       37.3047  0.0528\n",
      "     54       37.3047  0.0528\n",
      "     55       37.3047  0.0518\n",
      "     56       37.3047  0.0518\n",
      "     57       37.3047  0.0519\n",
      "     58       37.3047  0.0519\n",
      "     59       37.3047  0.0509\n",
      "     60       37.3047  0.0539\n",
      "     61       37.3047  0.0532\n",
      "     62       37.3047  0.0523\n",
      "     63       37.3047  0.0519\n",
      "     64       37.3047  0.0519\n",
      "     65       37.3047  0.0529\n",
      "     66       37.3047  0.0519\n",
      "     67       37.3047  0.0529\n",
      "     68       37.3047  0.0519\n",
      "     69       37.3047  0.0539\n",
      "     70       37.3047  0.0548\n",
      "     71       37.3047  0.0582\n",
      "     72       37.3047  0.0803\n",
      "     73       37.3047  0.0618\n",
      "     74       37.3047  0.0628\n",
      "     75       37.3047  0.0628\n",
      "     76       37.3047  0.0628\n",
      "     77       37.3047  0.0578\n",
      "     78       37.3047  0.0539\n",
      "     79       37.3047  0.0529\n",
      "     80       37.3047  0.0519\n",
      "     81       37.3047  0.0564\n",
      "     82       37.3047  0.0588\n",
      "     83       37.3047  0.0618\n",
      "     84       37.3047  0.0558\n",
      "     85       37.3047  0.0564\n",
      "     86       37.3047  0.0549\n",
      "     87       37.3047  0.0559\n",
      "     88       37.3047  0.0559\n",
      "     89       37.3047  0.0568\n",
      "     90       37.3047  0.0529\n",
      "     91       37.3047  0.0528\n",
      "     92       37.3047  0.0559\n",
      "     93       37.3047  0.0558\n",
      "     94       37.3047  0.0559\n",
      "     95       37.3047  0.0549\n",
      "     96       37.3047  0.0578\n",
      "     97       37.3047  0.0529\n",
      "     98       37.3047  0.0643\n",
      "     99       37.3047  0.0539\n",
      "    100       37.3047  0.0519\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       37.3047  0.0509\n",
      "      2       37.3047  0.0537\n",
      "      3       37.3047  0.0519\n",
      "      4       37.3047  0.0499\n",
      "      5       37.3047  0.0529\n",
      "      6       37.3047  0.0519\n",
      "      7       37.3047  0.0529\n",
      "      8       37.3047  0.0518\n",
      "      9       37.3047  0.0529\n",
      "     10       37.3047  0.0508\n",
      "     11       37.3047  0.0509\n",
      "     12       37.3047  0.0519\n",
      "     13       37.3047  0.0519\n",
      "     14       37.3047  0.0519\n",
      "     15       37.3047  0.0519\n",
      "     16       37.3047  0.0529\n",
      "     17       37.3047  0.0529\n",
      "     18       37.3047  0.0519\n",
      "     19       37.3047  0.0519\n",
      "     20       37.3047  0.0529\n",
      "     21       37.3047  0.0519\n",
      "     22       37.3047  0.0529\n",
      "     23       37.3047  0.0519\n",
      "     24       37.3047  0.0519\n",
      "     25       37.3047  0.0519\n",
      "     26       37.3047  0.0509\n",
      "     27       37.3047  0.0529\n",
      "     28       37.3047  0.0518\n",
      "     29       37.3047  0.0529\n",
      "     30       37.3047  0.0518\n",
      "     31       37.3047  0.0519\n",
      "     32       37.3047  0.0508\n",
      "     33       37.3047  0.0549\n",
      "     34       37.3047  0.0529\n",
      "     35       37.3047  0.0618\n",
      "     36       37.3047  0.0558\n",
      "     37       37.3047  0.0568\n",
      "     38       37.3047  0.0539\n",
      "     39       37.3047  0.0529\n",
      "     40       37.3047  0.0509\n",
      "     41       37.3047  0.0559\n",
      "     42       37.3047  0.0578\n",
      "     43       37.3047  0.0559\n",
      "     44       37.3047  0.0549\n",
      "     45       37.3047  0.0576\n",
      "     46       37.3047  0.0558\n",
      "     47       37.3047  0.0534\n",
      "     48       37.3047  0.0549\n",
      "     49       37.3047  0.0529\n",
      "     50       37.3047  0.0509\n",
      "     51       37.3047  0.0529\n",
      "     52       37.3047  0.0549\n",
      "     53       37.3047  0.0524\n",
      "     54       37.3047  0.0551\n",
      "     55       37.3047  0.0509\n",
      "     56       37.3047  0.0552\n",
      "     57       37.3047  0.0489\n",
      "     58       37.3047  0.0548\n",
      "     59       37.3047  0.1202\n",
      "     60       37.3047  0.0578\n",
      "     61       37.3047  0.0559\n",
      "     62       37.3047  0.0608\n",
      "     63       37.3047  0.0539\n",
      "     64       37.3047  0.0549\n",
      "     65       37.3047  0.0529\n",
      "     66       37.3047  0.0538\n",
      "     67       37.3047  0.0539\n",
      "     68       37.3047  0.0539\n",
      "     69       37.3047  0.0519\n",
      "     70       37.3047  0.0529\n",
      "     71       37.3047  0.0533\n",
      "     72       37.3047  0.0568\n",
      "     73       37.3047  0.0529\n",
      "     74       37.3047  0.0572\n",
      "     75       37.3047  0.0539\n",
      "     76       37.3047  0.0579\n",
      "     77       37.3047  0.0589\n",
      "     78       37.3047  0.0529\n",
      "     79       37.3047  0.0509\n",
      "     80       37.3047  0.0514\n",
      "     81       37.3047  0.0529\n",
      "     82       37.3047  0.0519\n",
      "     83       37.3047  0.0519\n",
      "     84       37.3047  0.0519\n",
      "     85       37.3047  0.0529\n",
      "     86       37.3047  0.0529\n",
      "     87       37.3047  0.0529\n",
      "     88       37.3047  0.0529\n",
      "     89       37.3047  0.0529\n",
      "     90       37.3047  0.0519\n",
      "     91       37.3047  0.0519\n",
      "     92       37.3047  0.0598\n",
      "     93       37.3047  0.0619\n",
      "     94       37.3047  0.0688\n",
      "     95       37.3047  0.0658\n",
      "     96       37.3047  0.0638\n",
      "     97       37.3047  0.0618\n",
      "     98       37.3047  0.0578\n",
      "     99       37.3047  0.0559\n",
      "    100       37.3047  0.0534\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       37.3047  0.0549\n",
      "      2       37.3047  0.0558\n",
      "      3       37.3047  0.0568\n",
      "      4       37.3047  0.0568\n",
      "      5       37.3047  0.0549\n",
      "      6       37.3047  0.0519\n",
      "      7       37.3047  0.0519\n",
      "      8       37.3047  0.0519\n",
      "      9       37.3047  0.0519\n",
      "     10       37.3047  0.0539\n",
      "     11       37.3047  0.0519\n",
      "     12       37.3047  0.0529\n",
      "     13       37.3047  0.0529\n",
      "     14       37.3047  0.0529\n",
      "     15       37.3047  0.0554\n",
      "     16       37.3047  0.0568\n",
      "     17       37.3047  0.0551\n",
      "     18       37.3047  0.0559\n",
      "     19       37.3047  0.0559\n",
      "     20       37.3047  0.0549\n",
      "     21       37.3047  0.0539\n",
      "     22       37.3047  0.0519\n",
      "     23       37.3047  0.0539\n",
      "     24       37.3047  0.0529\n",
      "     25       37.3047  0.0524\n",
      "     26       37.3047  0.0539\n",
      "     27       37.3047  0.0544\n",
      "     28       37.3047  0.0539\n",
      "     29       37.3047  0.0544\n",
      "     30       37.3047  0.0524\n",
      "     31       37.3047  0.0519\n",
      "     32       37.3047  0.0528\n",
      "     33       37.3047  0.0549\n",
      "     34       37.3047  0.0588\n",
      "     35       37.3047  0.0588\n",
      "     36       37.3047  0.0559\n",
      "     37       37.3047  0.0529\n",
      "     38       37.3047  0.0519\n",
      "     39       37.3047  0.0539\n",
      "     40       37.3047  0.0549\n",
      "     41       37.3047  0.0539\n",
      "     42       37.3047  0.0529\n",
      "     43       37.3047  0.0529\n",
      "     44       37.3047  0.0529\n",
      "     45       37.3047  0.0533\n",
      "     46       37.3047  0.0529\n",
      "     47       37.3047  0.0519\n",
      "     48       37.3047  0.0522\n",
      "     49       37.3047  0.0529\n",
      "     50       37.3047  0.0530\n",
      "     51       37.3047  0.0539\n",
      "     52       37.3047  0.0529\n",
      "     53       37.3047  0.0539\n",
      "     54       37.3047  0.0527\n",
      "     55       37.3047  0.0539\n",
      "     56       37.3047  0.0535\n",
      "     57       37.3047  0.0529\n",
      "     58       37.3047  0.0519\n",
      "     59       37.3047  0.0529\n",
      "     60       37.3047  0.0538\n",
      "     61       37.3047  0.0539\n",
      "     62       37.3047  0.0548\n",
      "     63       37.3047  0.0539\n",
      "     64       37.3047  0.0548\n",
      "     65       37.3047  0.0549\n",
      "     66       37.3047  0.0568\n",
      "     67       37.3047  0.0529\n",
      "     68       37.3047  0.0539\n",
      "     69       37.3047  0.0529\n",
      "     70       37.3047  0.0548\n",
      "     71       37.3047  0.0538\n",
      "     72       37.3047  0.0529\n",
      "     73       37.3047  0.0538\n",
      "     74       37.3047  0.0549\n",
      "     75       37.3047  0.0558\n",
      "     76       37.3047  0.0539\n",
      "     77       37.3047  0.0545\n",
      "     78       37.3047  0.0541\n",
      "     79       37.3047  0.0539\n",
      "     80       37.3047  0.0559\n",
      "     81       37.3047  0.0549\n",
      "     82       37.3047  0.0538\n",
      "     83       37.3047  0.0538\n",
      "     84       37.3047  0.0559\n",
      "     85       37.3047  0.0549\n",
      "     86       37.3047  0.0548\n",
      "     87       37.3047  0.0539\n",
      "     88       37.3047  0.0549\n",
      "     89       37.3047  0.0539\n",
      "     90       37.3047  0.0539\n",
      "     91       37.3047  0.0528\n",
      "     92       37.3047  0.0557\n",
      "     93       37.3047  0.0529\n",
      "     94       37.3047  0.0549\n",
      "     95       37.3047  0.0548\n",
      "     96       37.3047  0.0551\n",
      "     97       37.3047  0.0549\n",
      "     98       37.3047  0.0539\n",
      "     99       37.3047  0.0554\n",
      "    100       37.3047  0.0539\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       37.2320  0.0529\n",
      "      2       37.2320  0.0549\n",
      "      3       37.2320  0.0539\n",
      "      4       37.2320  0.0519\n",
      "      5       37.2320  0.0519\n",
      "      6       37.2320  0.0529\n",
      "      7       37.2320  0.0539\n",
      "      8       37.2320  0.0539\n",
      "      9       37.2320  0.0519\n",
      "     10       37.2320  0.0549\n",
      "     11       37.2320  0.0529\n",
      "     12       37.2320  0.0529\n",
      "     13       37.2320  0.0529\n",
      "     14       37.2320  0.0533\n",
      "     15       37.2320  0.0529\n",
      "     16       37.2320  0.0529\n",
      "     17       37.2320  0.0539\n",
      "     18       37.2320  0.0525\n",
      "     19       37.2320  0.0529\n",
      "     20       37.2320  0.0519\n",
      "     21       37.2320  0.0529\n",
      "     22       37.2320  0.0528\n",
      "     23       37.2320  0.0530\n",
      "     24       37.2320  0.0519\n",
      "     25       37.2320  0.0554\n",
      "     26       37.2320  0.0529\n",
      "     27       37.2320  0.0549\n",
      "     28       37.2320  0.0539\n",
      "     29       37.2320  0.0539\n",
      "     30       37.2320  0.0529\n",
      "     31       37.2320  0.0539\n",
      "     32       37.2320  0.0529\n",
      "     33       37.2320  0.0548\n",
      "     34       37.2320  0.0539\n",
      "     35       37.2320  0.0529\n",
      "     36       37.2320  0.0548\n",
      "     37       37.2320  0.0539\n",
      "     38       37.2320  0.0539\n",
      "     39       37.2320  0.0549\n",
      "     40       37.2320  0.0539\n",
      "     41       37.2320  0.0539\n",
      "     42       37.2320  0.0529\n",
      "     43       37.2320  0.0539\n",
      "     44       37.2320  0.0538\n",
      "     45       37.2320  0.0529\n",
      "     46       37.2320  0.0539\n",
      "     47       37.2320  0.0531\n",
      "     48       37.2320  0.0539\n",
      "     49       37.2320  0.0534\n",
      "     50       37.2320  0.0539\n",
      "     51       37.2320  0.0539\n",
      "     52       37.2320  0.0529\n",
      "     53       37.2320  0.0529\n",
      "     54       37.2320  0.0529\n",
      "     55       37.2320  0.0539\n",
      "     56       37.2320  0.0539\n",
      "     57       37.2320  0.0529\n",
      "     58       37.2320  0.0539\n",
      "     59       37.2320  0.0539\n",
      "     60       37.2320  0.0529\n",
      "     61       37.2320  0.0538\n",
      "     62       37.2320  0.0539\n",
      "     63       37.2320  0.0539\n",
      "     64       37.2320  0.0528\n",
      "     65       37.2320  0.0539\n",
      "     66       37.2320  0.0509\n",
      "     67       37.2320  0.0528\n",
      "     68       37.2320  0.1107\n",
      "     69       37.2320  0.1047\n",
      "     70       37.2320  0.0574\n",
      "     71       37.2320  0.0918\n",
      "     72       37.2320  0.0668\n",
      "     73       37.2320  0.0688\n",
      "     74       37.2320  0.0529\n",
      "     75       37.2320  0.0529\n",
      "     76       37.2320  0.0539\n",
      "     77       37.2320  0.0549\n",
      "     78       37.2320  0.0649\n",
      "     79       37.2320  0.0539\n",
      "     80       37.2320  0.0549\n",
      "     81       37.2320  0.0529\n",
      "     82       37.2320  0.0549\n",
      "     83       37.2320  0.0621\n",
      "     84       37.2320  0.0598\n",
      "     85       37.2320  0.0584\n",
      "     86       37.2320  0.0568\n",
      "     87       37.2320  0.0559\n",
      "     88       37.2320  0.0549\n",
      "     89       37.2320  0.0514\n",
      "     90       37.2320  0.0529\n",
      "     91       37.2320  0.0529\n",
      "     92       37.2320  0.0588\n",
      "     93       37.2320  0.0568\n",
      "     94       37.2320  0.0568\n",
      "     95       37.2320  0.0509\n",
      "     96       37.2320  0.0549\n",
      "     97       37.2320  0.0519\n",
      "     98       37.2320  0.0558\n",
      "     99       37.2320  0.0559\n",
      "    100       37.2320  0.0598\n"
     ]
    }
   ],
   "source": [
    "resultado = cross_val_score(nn_sklearn, datasetInput, datasetOuput, cv=10, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.61403509, 0.61403509, 0.63157895, 0.63157895, 0.63157895,\n",
       "       0.63157895, 0.63157895, 0.63157895, 0.63157895, 0.625     ])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RY0_dj7jxdmL"
   },
   "source": [
    "## Etapa 6: Dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tem que herda a classe nn.Module\n",
    "class model_nn_torch(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 30 ->16 ->16 ->1\n",
    "        self.dense0 = nn.Linear(30, 16)\n",
    "        torch.nn.init.uniform_(self.dense0.weight) # Pesos iniciando com distribuicao uniforme\n",
    "        self.activation0 = nn.ReLU()\n",
    "        self.dropout0 = nn.Dropout(20)\n",
    "        self.dense1 = nn.Linear(16, 16)\n",
    "        torch.nn.init.uniform_(self.dense1.weight)\n",
    "        self.activation1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(20) # Zerando 20% dos neurônios\n",
    "        self.dense2 = nn.Liner(16, 1)\n",
    "        torch.nn.init.uniform_(self.dense2.weight)\n",
    "        self.output = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, X):\n",
    "        X = self.dense0(X)\n",
    "        X = self.activation0(X)\n",
    "        X = self.dropout0(X)\n",
    "        X = self.dense1(X)\n",
    "        X = self.activation1(X)\n",
    "        X = self.dropout1(X)\n",
    "        X = self.dense2(X)\n",
    "        X = self.output(X)\n",
    "        return X\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       37.1094  0.0598\n",
      "      2       37.1094  0.0628\n",
      "      3       37.1094  0.0539\n",
      "      4       37.1094  0.0499\n",
      "      5       37.1094  0.0549\n",
      "      6       37.1094  0.0539\n",
      "      7       37.1094  0.0519\n",
      "      8       37.1094  0.0499\n",
      "      9       37.1094  0.0509\n",
      "     10       37.1094  0.0509\n",
      "     11       37.1094  0.0509\n",
      "     12       37.1094  0.0529\n",
      "     13       37.1094  0.0549\n",
      "     14       37.1094  0.0519\n",
      "     15       37.1094  0.0549\n",
      "     16       37.1094  0.0549\n",
      "     17       37.1094  0.0529\n",
      "     18       37.1094  0.0549\n",
      "     19       37.1094  0.0539\n",
      "     20       37.1094  0.0539\n",
      "     21       37.1094  0.0628\n",
      "     22       37.1094  0.0578\n",
      "     23       37.1094  0.0549\n",
      "     24       37.1094  0.0549\n",
      "     25       37.1094  0.0568\n",
      "     26       37.1094  0.0519\n",
      "     27       37.1094  0.0499\n",
      "     28       37.1094  0.0499\n",
      "     29       37.1094  0.0489\n",
      "     30       37.1094  0.0499\n",
      "     31       37.1094  0.0559\n",
      "     32       37.1094  0.0559\n",
      "     33       37.1094  0.0558\n",
      "     34       37.1094  0.0539\n",
      "     35       37.1094  0.0528\n",
      "     36       37.1094  0.0519\n",
      "     37       37.1094  0.0509\n",
      "     38       37.1094  0.0509\n",
      "     39       37.1094  0.0529\n",
      "     40       37.1094  0.0588\n",
      "     41       37.1094  0.0549\n",
      "     42       37.1094  0.0559\n",
      "     43       37.1094  0.0539\n",
      "     44       37.1094  0.0509\n",
      "     45       37.1094  0.0499\n",
      "     46       37.1094  0.0509\n",
      "     47       37.1094  0.0509\n",
      "     48       37.1094  0.0519\n",
      "     49       37.1094  0.0519\n",
      "     50       37.1094  0.0509\n",
      "     51       37.1094  0.0513\n",
      "     52       37.1094  0.0519\n",
      "     53       37.1094  0.0523\n",
      "     54       37.1094  0.0519\n",
      "     55       37.1094  0.0519\n",
      "     56       37.1094  0.0529\n",
      "     57       37.1094  0.0509\n",
      "     58       37.1094  0.0519\n",
      "     59       37.1094  0.0519\n",
      "     60       37.1094  0.0519\n",
      "     61       37.1094  0.0524\n",
      "     62       37.1094  0.0529\n",
      "     63       37.1094  0.0519\n",
      "     64       37.1094  0.0519\n",
      "     65       37.1094  0.0509\n",
      "     66       37.1094  0.0509\n",
      "     67       37.1094  0.0509\n",
      "     68       37.1094  0.0519\n",
      "     69       37.1094  0.0519\n",
      "     70       37.1094  0.0509\n",
      "     71       37.1094  0.0519\n",
      "     72       37.1094  0.0519\n",
      "     73       37.1094  0.0509\n",
      "     74       37.1094  0.0529\n",
      "     75       37.1094  0.0519\n",
      "     76       37.1094  0.0530\n",
      "     77       37.1094  0.0529\n",
      "     78       37.1094  0.0519\n",
      "     79       37.1094  0.0519\n",
      "     80       37.1094  0.0519\n",
      "     81       37.1094  0.0529\n",
      "     82       37.1094  0.0509\n",
      "     83       37.1094  0.0509\n",
      "     84       37.1094  0.0518\n",
      "     85       37.1094  0.0517\n",
      "     86       37.1094  0.0509\n",
      "     87       37.1094  0.0529\n",
      "     88       37.1094  0.0519\n",
      "     89       37.1094  0.0539\n",
      "     90       37.1094  0.0519\n",
      "     91       37.1094  0.0538\n",
      "     92       37.1094  0.0529\n",
      "     93       37.1094  0.0519\n",
      "     94       37.1094  0.0519\n",
      "     95       37.1094  0.0519\n",
      "     96       37.1094  0.0578\n",
      "     97       37.1094  0.0559\n",
      "     98       37.1094  0.0568\n",
      "     99       37.1094  0.0560\n",
      "    100       37.1094  0.0568\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       37.1094  0.0529\n",
      "      2       37.1094  0.0509\n",
      "      3       37.1094  0.0509\n",
      "      4       37.1094  0.0489\n",
      "      5       37.1094  0.0500\n",
      "      6       37.1094  0.0499\n",
      "      7       37.1094  0.0524\n",
      "      8       37.1094  0.0519\n",
      "      9       37.1094  0.0504\n",
      "     10       37.1094  0.0499\n",
      "     11       37.1094  0.0509\n",
      "     12       37.1094  0.0519\n",
      "     13       37.1094  0.0509\n",
      "     14       37.1094  0.0519\n",
      "     15       37.1094  0.0519\n",
      "     16       37.1094  0.0499\n",
      "     17       37.1094  0.0519\n",
      "     18       37.1094  0.0499\n",
      "     19       37.1094  0.0529\n",
      "     20       37.1094  0.0529\n",
      "     21       37.1094  0.0549\n",
      "     22       37.1094  0.0559\n",
      "     23       37.1094  0.0539\n",
      "     24       37.1094  0.0529\n",
      "     25       37.1094  0.0519\n",
      "     26       37.1094  0.0519\n",
      "     27       37.1094  0.0519\n",
      "     28       37.1094  0.0529\n",
      "     29       37.1094  0.0509\n",
      "     30       37.1094  0.0529\n",
      "     31       37.1094  0.0519\n",
      "     32       37.1094  0.0535\n",
      "     33       37.1094  0.0519\n",
      "     34       37.1094  0.0504\n",
      "     35       37.1094  0.0499\n",
      "     36       37.1094  0.0499\n",
      "     37       37.1094  0.0500\n",
      "     38       37.1094  0.0529\n",
      "     39       37.1094  0.0574\n",
      "     40       37.1094  0.0598\n",
      "     41       37.1094  0.0608\n",
      "     42       37.1094  0.0509\n",
      "     43       37.1094  0.0509\n",
      "     44       37.1094  0.0539\n",
      "     45       37.1094  0.0539\n",
      "     46       37.1094  0.0539\n",
      "     47       37.1094  0.0519\n",
      "     48       37.1094  0.0509\n",
      "     49       37.1094  0.0499\n",
      "     50       37.1094  0.0519\n",
      "     51       37.1094  0.0519\n",
      "     52       37.1094  0.0509\n",
      "     53       37.1094  0.0504\n",
      "     54       37.1094  0.0519\n",
      "     55       37.1094  0.0519\n",
      "     56       37.1094  0.0509\n",
      "     57       37.1094  0.0519\n",
      "     58       37.1094  0.0525\n",
      "     59       37.1094  0.0509\n",
      "     60       37.1094  0.0509\n",
      "     61       37.1094  0.0506\n",
      "     62       37.1094  0.0519\n",
      "     63       37.1094  0.0509\n",
      "     64       37.1094  0.0524\n",
      "     65       37.1094  0.0509\n",
      "     66       37.1094  0.0519\n",
      "     67       37.1094  0.0568\n",
      "     68       37.1094  0.0539\n",
      "     69       37.1094  0.0549\n",
      "     70       37.1094  0.0539\n",
      "     71       37.1094  0.0549\n",
      "     72       37.1094  0.0549\n",
      "     73       37.1094  0.0559\n",
      "     74       37.1094  0.0499\n",
      "     75       37.1094  0.0549\n",
      "     76       37.1094  0.0549\n",
      "     77       37.1094  0.0529\n",
      "     78       37.1094  0.0519\n",
      "     79       37.1094  0.0539\n",
      "     80       37.1094  0.0546\n",
      "     81       37.1094  0.0529\n",
      "     82       37.1094  0.0519\n",
      "     83       37.1094  0.0541\n",
      "     84       37.1094  0.0509\n",
      "     85       37.1094  0.0568\n",
      "     86       37.1094  0.0492\n",
      "     87       37.1094  0.0728\n",
      "     88       37.1094  0.0878\n",
      "     89       37.1094  0.0529\n",
      "     90       37.1094  0.0598\n",
      "     91       37.1094  0.0549\n",
      "     92       37.1094  0.0499\n",
      "     93       37.1094  0.0538\n",
      "     94       37.1094  0.0519\n",
      "     95       37.1094  0.0519\n",
      "     96       37.1094  0.0509\n",
      "     97       37.1094  0.0519\n",
      "     98       37.1094  0.0518\n",
      "     99       37.1094  0.0539\n",
      "    100       37.1094  0.0576\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       37.3047  0.0499\n",
      "      2       37.3047  0.0549\n",
      "      3       37.3047  0.0524\n",
      "      4       37.3047  0.0539\n",
      "      5       37.3047  0.0575\n",
      "      6       37.3047  0.0509\n",
      "      7       37.3047  0.0508\n",
      "      8       37.3047  0.0499\n",
      "      9       37.3047  0.0508\n",
      "     10       37.3047  0.0519\n",
      "     11       37.3047  0.0509\n",
      "     12       37.3047  0.0509\n",
      "     13       37.3047  0.0509\n",
      "     14       37.3047  0.0508\n",
      "     15       37.3047  0.0498\n",
      "     16       37.3047  0.0509\n",
      "     17       37.3047  0.0518\n",
      "     18       37.3047  0.0518\n",
      "     19       37.3047  0.0499\n",
      "     20       37.3047  0.0509\n",
      "     21       37.3047  0.0499\n",
      "     22       37.3047  0.0518\n",
      "     23       37.3047  0.0509\n",
      "     24       37.3047  0.0509\n",
      "     25       37.3047  0.0598\n",
      "     26       37.3047  0.0648\n",
      "     27       37.3047  0.0678\n",
      "     28       37.3047  0.0588\n",
      "     29       37.3047  0.0591\n",
      "     30       37.3047  0.0549\n",
      "     31       37.3047  0.0574\n",
      "     32       37.3047  0.0519\n",
      "     33       37.3047  0.0529\n",
      "     34       37.3047  0.0519\n",
      "     35       37.3047  0.0519\n",
      "     36       37.3047  0.0519\n",
      "     37       37.3047  0.0529\n",
      "     38       37.3047  0.0519\n",
      "     39       37.3047  0.0529\n",
      "     40       37.3047  0.0519\n",
      "     41       37.3047  0.0529\n",
      "     42       37.3047  0.0519\n",
      "     43       37.3047  0.0519\n",
      "     44       37.3047  0.0519\n",
      "     45       37.3047  0.0519\n",
      "     46       37.3047  0.0519\n",
      "     47       37.3047  0.0519\n",
      "     48       37.3047  0.0529\n",
      "     49       37.3047  0.0519\n",
      "     50       37.3047  0.0539\n",
      "     51       37.3047  0.0529\n",
      "     52       37.3047  0.0527\n",
      "     53       37.3047  0.0509\n",
      "     54       37.3047  0.0520\n",
      "     55       37.3047  0.0519\n",
      "     56       37.3047  0.0539\n",
      "     57       37.3047  0.0539\n",
      "     58       37.3047  0.0519\n",
      "     59       37.3047  0.0529\n",
      "     60       37.3047  0.0519\n",
      "     61       37.3047  0.0509\n",
      "     62       37.3047  0.0539\n",
      "     63       37.3047  0.0539\n",
      "     64       37.3047  0.0539\n",
      "     65       37.3047  0.0529\n",
      "     66       37.3047  0.0529\n",
      "     67       37.3047  0.0568\n",
      "     68       37.3047  0.0519\n",
      "     69       37.3047  0.0539\n",
      "     70       37.3047  0.0519\n",
      "     71       37.3047  0.0519\n",
      "     72       37.3047  0.0523\n",
      "     73       37.3047  0.0529\n",
      "     74       37.3047  0.0519\n",
      "     75       37.3047  0.0529\n",
      "     76       37.3047  0.0529\n",
      "     77       37.3047  0.0519\n",
      "     78       37.3047  0.0534\n",
      "     79       37.3047  0.0539\n",
      "     80       37.3047  0.0529\n",
      "     81       37.3047  0.0529\n",
      "     82       37.3047  0.0529\n",
      "     83       37.3047  0.0529\n",
      "     84       37.3047  0.0529\n",
      "     85       37.3047  0.0539\n",
      "     86       37.3047  0.0529\n",
      "     87       37.3047  0.0529\n",
      "     88       37.3047  0.0549\n",
      "     89       37.3047  0.0529\n",
      "     90       37.3047  0.0529\n",
      "     91       37.3047  0.0529\n",
      "     92       37.3047  0.0519\n",
      "     93       37.3047  0.0529\n",
      "     94       37.3047  0.0529\n",
      "     95       37.3047  0.0539\n",
      "     96       37.3047  0.0533\n",
      "     97       37.3047  0.0544\n",
      "     98       37.3047  0.0539\n",
      "     99       37.3047  0.0529\n",
      "    100       37.3047  0.0549\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       37.3047  0.0499\n",
      "      2       37.3047  0.0539\n",
      "      3       37.3047  0.0519\n",
      "      4       37.3047  0.0519\n",
      "      5       37.3047  0.0539\n",
      "      6       37.3047  0.0519\n",
      "      7       37.3047  0.0549\n",
      "      8       37.3047  0.0539\n",
      "      9       37.3047  0.0529\n",
      "     10       37.3047  0.0529\n",
      "     11       37.3047  0.0529\n",
      "     12       37.3047  0.0514\n",
      "     13       37.3047  0.0509\n",
      "     14       37.3047  0.0509\n",
      "     15       37.3047  0.0509\n",
      "     16       37.3047  0.0510\n",
      "     17       37.3047  0.0519\n",
      "     18       37.3047  0.0519\n",
      "     19       37.3047  0.0519\n",
      "     20       37.3047  0.0519\n",
      "     21       37.3047  0.0519\n",
      "     22       37.3047  0.0529\n",
      "     23       37.3047  0.0517\n",
      "     24       37.3047  0.0539\n",
      "     25       37.3047  0.0509\n",
      "     26       37.3047  0.0519\n",
      "     27       37.3047  0.0509\n",
      "     28       37.3047  0.0529\n",
      "     29       37.3047  0.0509\n",
      "     30       37.3047  0.0519\n",
      "     31       37.3047  0.0529\n",
      "     32       37.3047  0.0529\n",
      "     33       37.3047  0.0519\n",
      "     34       37.3047  0.0529\n",
      "     35       37.3047  0.0528\n",
      "     36       37.3047  0.0529\n",
      "     37       37.3047  0.0538\n",
      "     38       37.3047  0.0529\n",
      "     39       37.3047  0.0519\n",
      "     40       37.3047  0.0523\n",
      "     41       37.3047  0.0519\n",
      "     42       37.3047  0.0519\n",
      "     43       37.3047  0.0529\n",
      "     44       37.3047  0.0519\n",
      "     45       37.3047  0.0519\n",
      "     46       37.3047  0.0519\n",
      "     47       37.3047  0.0519\n",
      "     48       37.3047  0.0529\n",
      "     49       37.3047  0.0529\n",
      "     50       37.3047  0.0529\n",
      "     51       37.3047  0.0519\n",
      "     52       37.3047  0.0519\n",
      "     53       37.3047  0.0519\n",
      "     54       37.3047  0.0519\n",
      "     55       37.3047  0.0519\n",
      "     56       37.3047  0.0529\n",
      "     57       37.3047  0.0529\n",
      "     58       37.3047  0.0539\n",
      "     59       37.3047  0.0528\n",
      "     60       37.3047  0.0529\n",
      "     61       37.3047  0.0539\n",
      "     62       37.3047  0.0529\n",
      "     63       37.3047  0.0529\n",
      "     64       37.3047  0.0509\n",
      "     65       37.3047  0.0515\n",
      "     66       37.3047  0.0519\n",
      "     67       37.3047  0.0529\n",
      "     68       37.3047  0.0539\n",
      "     69       37.3047  0.0519\n",
      "     70       37.3047  0.0529\n",
      "     71       37.3047  0.0529\n",
      "     72       37.3047  0.0519\n",
      "     73       37.3047  0.0529\n",
      "     74       37.3047  0.0534\n",
      "     75       37.3047  0.0539\n",
      "     76       37.3047  0.0529\n",
      "     77       37.3047  0.0529\n",
      "     78       37.3047  0.0539\n",
      "     79       37.3047  0.0529\n",
      "     80       37.3047  0.0529\n",
      "     81       37.3047  0.0539\n",
      "     82       37.3047  0.0529\n",
      "     83       37.3047  0.0519\n",
      "     84       37.3047  0.0529\n",
      "     85       37.3047  0.0529\n",
      "     86       37.3047  0.0529\n",
      "     87       37.3047  0.0539\n",
      "     88       37.3047  0.0529\n",
      "     89       37.3047  0.0529\n",
      "     90       37.3047  0.0529\n",
      "     91       37.3047  0.0529\n",
      "     92       37.3047  0.0539\n",
      "     93       37.3047  0.0529\n",
      "     94       37.3047  0.0551\n",
      "     95       37.3047  0.0544\n",
      "     96       37.3047  0.0539\n",
      "     97       37.3047  0.0534\n",
      "     98       37.3047  0.0539\n",
      "     99       37.3047  0.0539\n",
      "    100       37.3047  0.0539\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       37.3047  0.0509\n",
      "      2       37.3047  0.0598\n",
      "      3       37.3047  0.0509\n",
      "      4       37.3047  0.0529\n",
      "      5       37.3047  0.0529\n",
      "      6       37.3047  0.0529\n",
      "      7       37.3047  0.0529\n",
      "      8       37.3047  0.0519\n",
      "      9       37.3047  0.0519\n",
      "     10       37.3047  0.0529\n",
      "     11       37.3047  0.0538\n",
      "     12       37.3047  0.0528\n",
      "     13       37.3047  0.0519\n",
      "     14       37.3047  0.0519\n",
      "     15       37.3047  0.0529\n",
      "     16       37.3047  0.0539\n",
      "     17       37.3047  0.0514\n",
      "     18       37.3047  0.0519\n",
      "     19       37.3047  0.0529\n",
      "     20       37.3047  0.0524\n",
      "     21       37.3047  0.0529\n",
      "     22       37.3047  0.0520\n",
      "     23       37.3047  0.0519\n",
      "     24       37.3047  0.0524\n",
      "     25       37.3047  0.0522\n",
      "     26       37.3047  0.0509\n",
      "     27       37.3047  0.0519\n",
      "     28       37.3047  0.0519\n",
      "     29       37.3047  0.0519\n",
      "     30       37.3047  0.0509\n",
      "     31       37.3047  0.0519\n",
      "     32       37.3047  0.0529\n",
      "     33       37.3047  0.0529\n",
      "     34       37.3047  0.0519\n",
      "     35       37.3047  0.0519\n",
      "     36       37.3047  0.0539\n",
      "     37       37.3047  0.0519\n",
      "     38       37.3047  0.0539\n",
      "     39       37.3047  0.0519\n",
      "     40       37.3047  0.0509\n",
      "     41       37.3047  0.0519\n",
      "     42       37.3047  0.0534\n",
      "     43       37.3047  0.0519\n",
      "     44       37.3047  0.0529\n",
      "     45       37.3047  0.0539\n",
      "     46       37.3047  0.0529\n",
      "     47       37.3047  0.0519\n",
      "     48       37.3047  0.0529\n",
      "     49       37.3047  0.0519\n",
      "     50       37.3047  0.0529\n",
      "     51       37.3047  0.0519\n",
      "     52       37.3047  0.0529\n",
      "     53       37.3047  0.0529\n",
      "     54       37.3047  0.0549\n",
      "     55       37.3047  0.0519\n",
      "     56       37.3047  0.0529\n",
      "     57       37.3047  0.0539\n",
      "     58       37.3047  0.0529\n",
      "     59       37.3047  0.0539\n",
      "     60       37.3047  0.0539\n",
      "     61       37.3047  0.0539\n",
      "     62       37.3047  0.0519\n",
      "     63       37.3047  0.0529\n",
      "     64       37.3047  0.0534\n",
      "     65       37.3047  0.0539\n",
      "     66       37.3047  0.0529\n",
      "     67       37.3047  0.0529\n",
      "     68       37.3047  0.0529\n",
      "     69       37.3047  0.0529\n",
      "     70       37.3047  0.0529\n",
      "     71       37.3047  0.0539\n",
      "     72       37.3047  0.0519\n",
      "     73       37.3047  0.0530\n",
      "     74       37.3047  0.0529\n",
      "     75       37.3047  0.0529\n",
      "     76       37.3047  0.0519\n",
      "     77       37.3047  0.0539\n",
      "     78       37.3047  0.0529\n",
      "     79       37.3047  0.0529\n",
      "     80       37.3047  0.0549\n",
      "     81       37.3047  0.0539\n",
      "     82       37.3047  0.0549\n",
      "     83       37.3047  0.0539\n",
      "     84       37.3047  0.0539\n",
      "     85       37.3047  0.0529\n",
      "     86       37.3047  0.0529\n",
      "     87       37.3047  0.0543\n",
      "     88       37.3047  0.0529\n",
      "     89       37.3047  0.0539\n",
      "     90       37.3047  0.0536\n",
      "     91       37.3047  0.0529\n",
      "     92       37.3047  0.0529\n",
      "     93       37.3047  0.0523\n",
      "     94       37.3047  0.0539\n",
      "     95       37.3047  0.0549\n",
      "     96       37.3047  0.0543\n",
      "     97       37.3047  0.0529\n",
      "     98       37.3047  0.0549\n",
      "     99       37.3047  0.0539\n",
      "    100       37.3047  0.0519\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       37.3047  0.0499\n",
      "      2       37.3047  0.0529\n",
      "      3       37.3047  0.0509\n",
      "      4       37.3047  0.0529\n",
      "      5       37.3047  0.0529\n",
      "      6       37.3047  0.0519\n",
      "      7       37.3047  0.0509\n",
      "      8       37.3047  0.0519\n",
      "      9       37.3047  0.0529\n",
      "     10       37.3047  0.0524\n",
      "     11       37.3047  0.0519\n",
      "     12       37.3047  0.0521\n",
      "     13       37.3047  0.0529\n",
      "     14       37.3047  0.0520\n",
      "     15       37.3047  0.0539\n",
      "     16       37.3047  0.0524\n",
      "     17       37.3047  0.0519\n",
      "     18       37.3047  0.0519\n",
      "     19       37.3047  0.0509\n",
      "     20       37.3047  0.0529\n",
      "     21       37.3047  0.0509\n",
      "     22       37.3047  0.0519\n",
      "     23       37.3047  0.0525\n",
      "     24       37.3047  0.0509\n",
      "     25       37.3047  0.0519\n",
      "     26       37.3047  0.0514\n",
      "     27       37.3047  0.0519\n",
      "     28       37.3047  0.0520\n",
      "     29       37.3047  0.0519\n",
      "     30       37.3047  0.0544\n",
      "     31       37.3047  0.0521\n",
      "     32       37.3047  0.0509\n",
      "     33       37.3047  0.0514\n",
      "     34       37.3047  0.0519\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     35       37.3047  0.0529\n",
      "     36       37.3047  0.0519\n",
      "     37       37.3047  0.0519\n",
      "     38       37.3047  0.0499\n",
      "     39       37.3047  0.0519\n",
      "     40       37.3047  0.0519\n",
      "     41       37.3047  0.0509\n",
      "     42       37.3047  0.0529\n",
      "     43       37.3047  0.0519\n",
      "     44       37.3047  0.0539\n",
      "     45       37.3047  0.0539\n",
      "     46       37.3047  0.0529\n",
      "     47       37.3047  0.0509\n",
      "     48       37.3047  0.0529\n",
      "     49       37.3047  0.0524\n",
      "     50       37.3047  0.0509\n",
      "     51       37.3047  0.0509\n",
      "     52       37.3047  0.0519\n",
      "     53       37.3047  0.0519\n",
      "     54       37.3047  0.0514\n",
      "     55       37.3047  0.0519\n",
      "     56       37.3047  0.0506\n",
      "     57       37.3047  0.0519\n",
      "     58       37.3047  0.0529\n",
      "     59       37.3047  0.0529\n",
      "     60       37.3047  0.0529\n",
      "     61       37.3047  0.0519\n",
      "     62       37.3047  0.0529\n",
      "     63       37.3047  0.0529\n",
      "     64       37.3047  0.0529\n",
      "     65       37.3047  0.0529\n",
      "     66       37.3047  0.0539\n",
      "     67       37.3047  0.0529\n",
      "     68       37.3047  0.0529\n",
      "     69       37.3047  0.0529\n",
      "     70       37.3047  0.0529\n",
      "     71       37.3047  0.0532\n",
      "     72       37.3047  0.0529\n",
      "     73       37.3047  0.0519\n",
      "     74       37.3047  0.0514\n",
      "     75       37.3047  0.0529\n",
      "     76       37.3047  0.0529\n",
      "     77       37.3047  0.0529\n",
      "     78       37.3047  0.0533\n",
      "     79       37.3047  0.0529\n",
      "     80       37.3047  0.0514\n",
      "     81       37.3047  0.0512\n",
      "     82       37.3047  0.0519\n",
      "     83       37.3047  0.0512\n",
      "     84       37.3047  0.0519\n",
      "     85       37.3047  0.0529\n",
      "     86       37.3047  0.0519\n",
      "     87       37.3047  0.0529\n",
      "     88       37.3047  0.0529\n",
      "     89       37.3047  0.0549\n",
      "     90       37.3047  0.0529\n",
      "     91       37.3047  0.0529\n",
      "     92       37.3047  0.0529\n",
      "     93       37.3047  0.0539\n",
      "     94       37.3047  0.0519\n",
      "     95       37.3047  0.0529\n",
      "     96       37.3047  0.0529\n",
      "     97       37.3047  0.0519\n",
      "     98       37.3047  0.0516\n",
      "     99       37.3047  0.0529\n",
      "    100       37.3047  0.0529\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       37.3047  0.0578\n",
      "      2       37.3047  0.0509\n",
      "      3       37.3047  0.0513\n",
      "      4       37.3047  0.0529\n",
      "      5       37.3047  0.0509\n",
      "      6       37.3047  0.0529\n",
      "      7       37.3047  0.0519\n",
      "      8       37.3047  0.0539\n",
      "      9       37.3047  0.0529\n",
      "     10       37.3047  0.0509\n",
      "     11       37.3047  0.0509\n",
      "     12       37.3047  0.0519\n",
      "     13       37.3047  0.0509\n",
      "     14       37.3047  0.0509\n",
      "     15       37.3047  0.0519\n",
      "     16       37.3047  0.0529\n",
      "     17       37.3047  0.0519\n",
      "     18       37.3047  0.0519\n",
      "     19       37.3047  0.0513\n",
      "     20       37.3047  0.0519\n",
      "     21       37.3047  0.0519\n",
      "     22       37.3047  0.0529\n",
      "     23       37.3047  0.0534\n",
      "     24       37.3047  0.0534\n",
      "     25       37.3047  0.0519\n",
      "     26       37.3047  0.0519\n",
      "     27       37.3047  0.0509\n",
      "     28       37.3047  0.0509\n",
      "     29       37.3047  0.0509\n",
      "     30       37.3047  0.0519\n",
      "     31       37.3047  0.0502\n",
      "     32       37.3047  0.0519\n",
      "     33       37.3047  0.0531\n",
      "     34       37.3047  0.0519\n",
      "     35       37.3047  0.0509\n",
      "     36       37.3047  0.0529\n",
      "     37       37.3047  0.0509\n",
      "     38       37.3047  0.0529\n",
      "     39       37.3047  0.0519\n",
      "     40       37.3047  0.0519\n",
      "     41       37.3047  0.0539\n",
      "     42       37.3047  0.0529\n",
      "     43       37.3047  0.0519\n",
      "     44       37.3047  0.0509\n",
      "     45       37.3047  0.0529\n",
      "     46       37.3047  0.0539\n",
      "     47       37.3047  0.0519\n",
      "     48       37.3047  0.0519\n",
      "     49       37.3047  0.0519\n",
      "     50       37.3047  0.0519\n",
      "     51       37.3047  0.0519\n",
      "     52       37.3047  0.0529\n",
      "     53       37.3047  0.0539\n",
      "     54       37.3047  0.0548\n",
      "     55       37.3047  0.0539\n",
      "     56       37.3047  0.0529\n",
      "     57       37.3047  0.0542\n",
      "     58       37.3047  0.0539\n",
      "     59       37.3047  0.0538\n",
      "     60       37.3047  0.0519\n",
      "     61       37.3047  0.0529\n",
      "     62       37.3047  0.0519\n",
      "     63       37.3047  0.0539\n",
      "     64       37.3047  0.0529\n",
      "     65       37.3047  0.0539\n",
      "     66       37.3047  0.0539\n",
      "     67       37.3047  0.0529\n",
      "     68       37.3047  0.0529\n",
      "     69       37.3047  0.0529\n",
      "     70       37.3047  0.0539\n",
      "     71       37.3047  0.0529\n",
      "     72       37.3047  0.0529\n",
      "     73       37.3047  0.0519\n",
      "     74       37.3047  0.0529\n",
      "     75       37.3047  0.0529\n",
      "     76       37.3047  0.0529\n",
      "     77       37.3047  0.0519\n",
      "     78       37.3047  0.0570\n",
      "     79       37.3047  0.0499\n",
      "     80       37.3047  0.0638\n",
      "     81       37.3047  0.1247\n",
      "     82       37.3047  0.0608\n",
      "     83       37.3047  0.0748\n",
      "     84       37.3047  0.0798\n",
      "     85       37.3047  0.0818\n",
      "     86       37.3047  0.0559\n",
      "     87       37.3047  0.0549\n",
      "     88       37.3047  0.0538\n",
      "     89       37.3047  0.0519\n",
      "     90       37.3047  0.0519\n",
      "     91       37.3047  0.0531\n",
      "     92       37.3047  0.0559\n",
      "     93       37.3047  0.0519\n",
      "     94       37.3047  0.0539\n",
      "     95       37.3047  0.0549\n",
      "     96       37.3047  0.0549\n",
      "     97       37.3047  0.0519\n",
      "     98       37.3047  0.0516\n",
      "     99       37.3047  0.0514\n",
      "    100       37.3047  0.0519\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       37.3047  0.0549\n",
      "      2       37.3047  0.0568\n",
      "      3       37.3047  0.0539\n",
      "      4       37.3047  0.0509\n",
      "      5       37.3047  0.0499\n",
      "      6       37.3047  0.0549\n",
      "      7       37.3047  0.0598\n",
      "      8       37.3047  0.0598\n",
      "      9       37.3047  0.0598\n",
      "     10       37.3047  0.0578\n",
      "     11       37.3047  0.0549\n",
      "     12       37.3047  0.0549\n",
      "     13       37.3047  0.0539\n",
      "     14       37.3047  0.0529\n",
      "     15       37.3047  0.0539\n",
      "     16       37.3047  0.0519\n",
      "     17       37.3047  0.0549\n",
      "     18       37.3047  0.0539\n",
      "     19       37.3047  0.0549\n",
      "     20       37.3047  0.0539\n",
      "     21       37.3047  0.0485\n",
      "     22       37.3047  0.0519\n",
      "     23       37.3047  0.0519\n",
      "     24       37.3047  0.0509\n",
      "     25       37.3047  0.0499\n",
      "     26       37.3047  0.0519\n",
      "     27       37.3047  0.0608\n",
      "     28       37.3047  0.0608\n",
      "     29       37.3047  0.0628\n",
      "     30       37.3047  0.0618\n",
      "     31       37.3047  0.0588\n",
      "     32       37.3047  0.0519\n",
      "     33       37.3047  0.0499\n",
      "     34       37.3047  0.0499\n",
      "     35       37.3047  0.0509\n",
      "     36       37.3047  0.0509\n",
      "     37       37.3047  0.0499\n",
      "     38       37.3047  0.0529\n",
      "     39       37.3047  0.0529\n",
      "     40       37.3047  0.0539\n",
      "     41       37.3047  0.0524\n",
      "     42       37.3047  0.0519\n",
      "     43       37.3047  0.0499\n",
      "     44       37.3047  0.0509\n",
      "     45       37.3047  0.0539\n",
      "     46       37.3047  0.0549\n",
      "     47       37.3047  0.0529\n",
      "     48       37.3047  0.0519\n",
      "     49       37.3047  0.0509\n",
      "     50       37.3047  0.0509\n",
      "     51       37.3047  0.0509\n",
      "     52       37.3047  0.0509\n",
      "     53       37.3047  0.0519\n",
      "     54       37.3047  0.0548\n",
      "     55       37.3047  0.0559\n",
      "     56       37.3047  0.0549\n",
      "     57       37.3047  0.0529\n",
      "     58       37.3047  0.0539\n",
      "     59       37.3047  0.0549\n",
      "     60       37.3047  0.0519\n",
      "     61       37.3047  0.0534\n",
      "     62       37.3047  0.0519\n",
      "     63       37.3047  0.0521\n",
      "     64       37.3047  0.0529\n",
      "     65       37.3047  0.0519\n",
      "     66       37.3047  0.0559\n",
      "     67       37.3047  0.0549\n",
      "     68       37.3047  0.0499\n",
      "     69       37.3047  0.0519\n",
      "     70       37.3047  0.0544\n",
      "     71       37.3047  0.0509\n",
      "     72       37.3047  0.0509\n",
      "     73       37.3047  0.0509\n",
      "     74       37.3047  0.0509\n",
      "     75       37.3047  0.0509\n",
      "     76       37.3047  0.0519\n",
      "     77       37.3047  0.0509\n",
      "     78       37.3047  0.0539\n",
      "     79       37.3047  0.0549\n",
      "     80       37.3047  0.0529\n",
      "     81       37.3047  0.0519\n",
      "     82       37.3047  0.0509\n",
      "     83       37.3047  0.0519\n",
      "     84       37.3047  0.0519\n",
      "     85       37.3047  0.0512\n",
      "     86       37.3047  0.0529\n",
      "     87       37.3047  0.0529\n",
      "     88       37.3047  0.0515\n",
      "     89       37.3047  0.0518\n",
      "     90       37.3047  0.0519\n",
      "     91       37.3047  0.0539\n",
      "     92       37.3047  0.0521\n",
      "     93       37.3047  0.0519\n",
      "     94       37.3047  0.0529\n",
      "     95       37.3047  0.0530\n",
      "     96       37.3047  0.0549\n",
      "     97       37.3047  0.0499\n",
      "     98       37.3047  0.0519\n",
      "     99       37.3047  0.0509\n",
      "    100       37.3047  0.0519\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       37.3047  0.0499\n",
      "      2       37.3047  0.0519\n",
      "      3       37.3047  0.0509\n",
      "      4       37.3047  0.0499\n",
      "      5       37.3047  0.0499\n",
      "      6       37.3047  0.0509\n",
      "      7       37.3047  0.0499\n",
      "      8       37.3047  0.0509\n",
      "      9       37.3047  0.0509\n",
      "     10       37.3047  0.0509\n",
      "     11       37.3047  0.0489\n",
      "     12       37.3047  0.0509\n",
      "     13       37.3047  0.0499\n",
      "     14       37.3047  0.0501\n",
      "     15       37.3047  0.0499\n",
      "     16       37.3047  0.0509\n",
      "     17       37.3047  0.0499\n",
      "     18       37.3047  0.0509\n",
      "     19       37.3047  0.0498\n",
      "     20       37.3047  0.0499\n",
      "     21       37.3047  0.0525\n",
      "     22       37.3047  0.0499\n",
      "     23       37.3047  0.0533\n",
      "     24       37.3047  0.0524\n",
      "     25       37.3047  0.0519\n",
      "     26       37.3047  0.0539\n",
      "     27       37.3047  0.0559\n",
      "     28       37.3047  0.0509\n",
      "     29       37.3047  0.0509\n",
      "     30       37.3047  0.0519\n",
      "     31       37.3047  0.0489\n",
      "     32       37.3047  0.0539\n",
      "     33       37.3047  0.0499\n",
      "     34       37.3047  0.0578\n",
      "     35       37.3047  0.0509\n",
      "     36       37.3047  0.0499\n",
      "     37       37.3047  0.0499\n",
      "     38       37.3047  0.0549\n",
      "     39       37.3047  0.0549\n",
      "     40       37.3047  0.0549\n",
      "     41       37.3047  0.0519\n",
      "     42       37.3047  0.0499\n",
      "     43       37.3047  0.0511\n",
      "     44       37.3047  0.0489\n",
      "     45       37.3047  0.0509\n",
      "     46       37.3047  0.0499\n",
      "     47       37.3047  0.0519\n",
      "     48       37.3047  0.0528\n",
      "     49       37.3047  0.0568\n",
      "     50       37.3047  0.0539\n",
      "     51       37.3047  0.0549\n",
      "     52       37.3047  0.0539\n",
      "     53       37.3047  0.0509\n",
      "     54       37.3047  0.0539\n",
      "     55       37.3047  0.0519\n",
      "     56       37.3047  0.0519\n",
      "     57       37.3047  0.0568\n",
      "     58       37.3047  0.0509\n",
      "     59       37.3047  0.0509\n",
      "     60       37.3047  0.0509\n",
      "     61       37.3047  0.0529\n",
      "     62       37.3047  0.0529\n",
      "     63       37.3047  0.0514\n",
      "     64       37.3047  0.0519\n",
      "     65       37.3047  0.0514\n",
      "     66       37.3047  0.0509\n",
      "     67       37.3047  0.0519\n",
      "     68       37.3047  0.0519\n",
      "     69       37.3047  0.0519\n",
      "     70       37.3047  0.0519\n",
      "     71       37.3047  0.0559\n",
      "     72       37.3047  0.0593\n",
      "     73       37.3047  0.0549\n",
      "     74       37.3047  0.0564\n",
      "     75       37.3047  0.0549\n",
      "     76       37.3047  0.0529\n",
      "     77       37.3047  0.0519\n",
      "     78       37.3047  0.0509\n",
      "     79       37.3047  0.0499\n",
      "     80       37.3047  0.0509\n",
      "     81       37.3047  0.0509\n",
      "     82       37.3047  0.0509\n",
      "     83       37.3047  0.0509\n",
      "     84       37.3047  0.0509\n",
      "     85       37.3047  0.0519\n",
      "     86       37.3047  0.0519\n",
      "     87       37.3047  0.0519\n",
      "     88       37.3047  0.0519\n",
      "     89       37.3047  0.0519\n",
      "     90       37.3047  0.0506\n",
      "     91       37.3047  0.0519\n",
      "     92       37.3047  0.0519\n",
      "     93       37.3047  0.0510\n",
      "     94       37.3047  0.0504\n",
      "     95       37.3047  0.0509\n",
      "     96       37.3047  0.0519\n",
      "     97       37.3047  0.0510\n",
      "     98       37.3047  0.0529\n",
      "     99       37.3047  0.0519\n",
      "    100       37.3047  0.0514\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       37.2320  0.0524\n",
      "      2       37.2320  0.0509\n",
      "      3       37.2320  0.0509\n",
      "      4       37.2320  0.0509\n",
      "      5       37.2320  0.0509\n",
      "      6       37.2320  0.0509\n",
      "      7       37.2320  0.0509\n",
      "      8       37.2320  0.0509\n",
      "      9       37.2320  0.0519\n",
      "     10       37.2320  0.0499\n",
      "     11       37.2320  0.0499\n",
      "     12       37.2320  0.0499\n",
      "     13       37.2320  0.0499\n",
      "     14       37.2320  0.0499\n",
      "     15       37.2320  0.0509\n",
      "     16       37.2320  0.0509\n",
      "     17       37.2320  0.0499\n",
      "     18       37.2320  0.0509\n",
      "     19       37.2320  0.0519\n",
      "     20       37.2320  0.0517\n",
      "     21       37.2320  0.0519\n",
      "     22       37.2320  0.0549\n",
      "     23       37.2320  0.0519\n",
      "     24       37.2320  0.0519\n",
      "     25       37.2320  0.0529\n",
      "     26       37.2320  0.0519\n",
      "     27       37.2320  0.0504\n",
      "     28       37.2320  0.0489\n",
      "     29       37.2320  0.0509\n",
      "     30       37.2320  0.0509\n",
      "     31       37.2320  0.0519\n",
      "     32       37.2320  0.0499\n",
      "     33       37.2320  0.0539\n",
      "     34       37.2320  0.0509\n",
      "     35       37.2320  0.0509\n",
      "     36       37.2320  0.0509\n",
      "     37       37.2320  0.0519\n",
      "     38       37.2320  0.0539\n",
      "     39       37.2320  0.0519\n",
      "     40       37.2320  0.0499\n",
      "     41       37.2320  0.0509\n",
      "     42       37.2320  0.0519\n",
      "     43       37.2320  0.0509\n",
      "     44       37.2320  0.0499\n",
      "     45       37.2320  0.0494\n",
      "     46       37.2320  0.0519\n",
      "     47       37.2320  0.0529\n",
      "     48       37.2320  0.0578\n",
      "     49       37.2320  0.0558\n",
      "     50       37.2320  0.0549\n",
      "     51       37.2320  0.0509\n",
      "     52       37.2320  0.0512\n",
      "     53       37.2320  0.0519\n",
      "     54       37.2320  0.0499\n",
      "     55       37.2320  0.0509\n",
      "     56       37.2320  0.0499\n",
      "     57       37.2320  0.0509\n",
      "     58       37.2320  0.0499\n",
      "     59       37.2320  0.0519\n",
      "     60       37.2320  0.0519\n",
      "     61       37.2320  0.0499\n",
      "     62       37.2320  0.0509\n",
      "     63       37.2320  0.0519\n",
      "     64       37.2320  0.0509\n",
      "     65       37.2320  0.0509\n",
      "     66       37.2320  0.0519\n",
      "     67       37.2320  0.0509\n",
      "     68       37.2320  0.0519\n",
      "     69       37.2320  0.0539\n",
      "     70       37.2320  0.0519\n",
      "     71       37.2320  0.0509\n",
      "     72       37.2320  0.0519\n",
      "     73       37.2320  0.0509\n",
      "     74       37.2320  0.0529\n",
      "     75       37.2320  0.0528\n",
      "     76       37.2320  0.0519\n",
      "     77       37.2320  0.0519\n",
      "     78       37.2320  0.0509\n",
      "     79       37.2320  0.0519\n",
      "     80       37.2320  0.0509\n",
      "     81       37.2320  0.0519\n",
      "     82       37.2320  0.0509\n",
      "     83       37.2320  0.0539\n",
      "     84       37.2320  0.0519\n",
      "     85       37.2320  0.0519\n",
      "     86       37.2320  0.0509\n",
      "     87       37.2320  0.0509\n",
      "     88       37.2320  0.0519\n",
      "     89       37.2320  0.0509\n",
      "     90       37.2320  0.0524\n",
      "     91       37.2320  0.0530\n",
      "     92       37.2320  0.0519\n",
      "     93       37.2320  0.0509\n",
      "     94       37.2320  0.0519\n",
      "     95       37.2320  0.0519\n",
      "     96       37.2320  0.0509\n",
      "     97       37.2320  0.0519\n",
      "     98       37.2320  0.0529\n",
      "     99       37.2320  0.0519\n",
      "    100       37.2320  0.0529\n"
     ]
    }
   ],
   "source": [
    "resultado = cross_val_score(nn_sklearn, datasetInput, datasetOuput, cv=10, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.61403509, 0.61403509, 0.63157895, 0.63157895, 0.63157895,\n",
       "       0.63157895, 0.63157895, 0.63157895, 0.63157895, 0.625     ])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultado"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOTOFNd2Bj3A4NAfd/kQGt/",
   "collapsed_sections": [],
   "name": "Projeto1 PyTorch.ipynb",
   "provenance": [
    {
     "file_id": "1m_i5cylLpDi0dEsra8ZOanWz9H0LYQJf",
     "timestamp": 1594991802708
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
